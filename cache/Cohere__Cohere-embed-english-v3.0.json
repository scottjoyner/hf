{
  "_id": "65439514ad865d6231e2d882",
  "id": "Cohere/Cohere-embed-english-v3.0",
  "private": false,
  "library_name": "transformers",
  "tags": [
    "transformers",
    "mteb",
    "model-index",
    "endpoints_compatible",
    "region:us"
  ],
  "downloads": 52,
  "likes": 46,
  "modelId": "Cohere/Cohere-embed-english-v3.0",
  "author": "Cohere",
  "sha": "a73b09960122f77a78083ff79084162793ed9c39",
  "lastModified": "2023-11-02T12:26:14.000Z",
  "gated": false,
  "disabled": false,
  "model-index": [
    {
      "name": "embed-english-v3.0",
      "results": [
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_counterfactual",
            "name": "MTEB AmazonCounterfactualClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "e8379541af4e31359cca9fbcf4b00f2671dba205"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 81.29850746268656,
              "verified": false
            },
            {
              "type": "ap",
              "value": 46.181772245676136,
              "verified": false
            },
            {
              "type": "f1",
              "value": 75.47731234579823,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_polarity",
            "name": "MTEB AmazonPolarityClassification",
            "config": "default",
            "split": "test",
            "revision": "e2d317d38cd51312af73b3d32a06d1a08b442046"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 95.61824999999999,
              "verified": false
            },
            {
              "type": "ap",
              "value": 93.22525741797098,
              "verified": false
            },
            {
              "type": "f1",
              "value": 95.61627312544859,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_reviews_multi",
            "name": "MTEB AmazonReviewsClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "1399c76144fd37290681b995c656ef9b2e06e26d"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 51.72,
              "verified": false
            },
            {
              "type": "f1",
              "value": 50.529480725642465,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "arguana",
            "name": "MTEB ArguAna",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 61.521,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/arxiv-clustering-p2p",
            "name": "MTEB ArxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "a122ad7f3f0291bf49cc6f4d32aa80929df69d5d"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 49.173332266218914,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/arxiv-clustering-s2s",
            "name": "MTEB ArxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "f910caf1a6075f7329cdf8c1a6135696f37dbd53"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 42.1800504937582,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/askubuntudupquestions-reranking",
            "name": "MTEB AskUbuntuDupQuestions",
            "config": "default",
            "split": "test",
            "revision": "2000358ca161889fa9c082cb41daa8dcfb161a54"
          },
          "metrics": [
            {
              "type": "map",
              "value": 61.69942465283367,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 73.8089741898606,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/biosses-sts",
            "name": "MTEB BIOSSES",
            "config": "default",
            "split": "test",
            "revision": "d3fb88f8f02e40887cd149695127462bbcf29b4a"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.1805709775319,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 83.50310749422796,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 83.57134970408762,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 83.50310749422796,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 83.422472116232,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 83.35611619312422,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/banking77",
            "name": "MTEB Banking77Classification",
            "config": "default",
            "split": "test",
            "revision": "0fd18e25b25c072e09e0d92ab615fda904d66300"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 85.52922077922078,
              "verified": false
            },
            {
              "type": "f1",
              "value": 85.48530911742581,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/biorxiv-clustering-p2p",
            "name": "MTEB BiorxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "65b79d1d13f80053f67aca9498d9402c2d9f1f40"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 40.95750155360001,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/biorxiv-clustering-s2s",
            "name": "MTEB BiorxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "258694dd0231531bc1fd9de6ceb52a0853c6d908"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 37.25334765305169,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackAndroidRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 50.037,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackEnglishRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 49.089,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackGamingRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 60.523,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackGisRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 39.293,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackMathematicaRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 30.414,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackPhysicsRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 43.662,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackProgrammersRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 43.667,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 41.53158333333334,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackStatsRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 35.258,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackTexRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 30.866,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackUnixRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 40.643,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackWebmastersRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 40.663,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackWordpressRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 34.264,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "climate-fever",
            "name": "MTEB ClimateFEVER",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 38.433,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "dbpedia-entity",
            "name": "MTEB DBPedia",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 43.36,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/emotion",
            "name": "MTEB EmotionClassification",
            "config": "default",
            "split": "test",
            "revision": "4f58c6b202a23cf9a4da393831edf4f9183cad37"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 51.574999999999996,
              "verified": false
            },
            {
              "type": "f1",
              "value": 46.84362123583929,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "fever",
            "name": "MTEB FEVER",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 88.966,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "fiqa",
            "name": "MTEB FiQA2018",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 42.189,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "hotpotqa",
            "name": "MTEB HotpotQA",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 70.723,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/imdb",
            "name": "MTEB ImdbClassification",
            "config": "default",
            "split": "test",
            "revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 93.56920000000001,
              "verified": false
            },
            {
              "type": "ap",
              "value": 90.56104192134326,
              "verified": false
            },
            {
              "type": "f1",
              "value": 93.56471146876505,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "msmarco",
            "name": "MTEB MSMARCO",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 42.931000000000004,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/mtop_domain",
            "name": "MTEB MTOPDomainClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 94.88372093023256,
              "verified": false
            },
            {
              "type": "f1",
              "value": 94.64417024711646,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/mtop_intent",
            "name": "MTEB MTOPIntentClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 76.52302781577748,
              "verified": false
            },
            {
              "type": "f1",
              "value": 59.52848723786157,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_massive_intent",
            "name": "MTEB MassiveIntentClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 73.84330867518494,
              "verified": false
            },
            {
              "type": "f1",
              "value": 72.18121296285702,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_massive_scenario",
            "name": "MTEB MassiveScenarioClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "7d571f92784cd94a019292a1f45445077d0ef634"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 78.73907195696033,
              "verified": false
            },
            {
              "type": "f1",
              "value": 78.86079300338558,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/medrxiv-clustering-p2p",
            "name": "MTEB MedrxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "e7a26af6f3ae46b30dde8737f02c07b1505bcc73"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 37.40673427491627,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/medrxiv-clustering-s2s",
            "name": "MTEB MedrxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "35191c8c0dca72d8ff3efcd72aa802307d469663"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 33.38936252583581,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/mind_small",
            "name": "MTEB MindSmallReranking",
            "config": "default",
            "split": "test",
            "revision": "3bdac13927fdc888b903db93b2ffdbd90b295a69"
          },
          "metrics": [
            {
              "type": "map",
              "value": 32.67317850167471,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 33.9334102169254,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "nfcorpus",
            "name": "MTEB NFCorpus",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 38.574000000000005,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "nq",
            "name": "MTEB NQ",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 61.556,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "quora",
            "name": "MTEB QuoraRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 88.722,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/reddit-clustering",
            "name": "MTEB RedditClustering",
            "config": "default",
            "split": "test",
            "revision": "24640382cdbf8abc73003fb0fa6d111a705499eb"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 58.45790556534654,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/reddit-clustering-p2p",
            "name": "MTEB RedditClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "282350215ef01743dc01b456c7f5241fa8937f16"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 66.35141658656822,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "scidocs",
            "name": "MTEB SCIDOCS",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 20.314,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sickr-sts",
            "name": "MTEB SICK-R",
            "config": "default",
            "split": "test",
            "revision": "a6ea5a8cab320b040a23452cc28066d9beae2cee"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.49945063881191,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 81.27177640994141,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 82.74613694646263,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 81.2717795980493,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 82.75268512220467,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 81.28362006796547,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts12-sts",
            "name": "MTEB STS12",
            "config": "default",
            "split": "test",
            "revision": "a0d554a64d88156834ff5ae9920b964011b16384"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 83.17562591888526,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 74.37099514810372,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 79.97392043583372,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 74.37103618585903,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 80.00641585184354,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 74.35403985608939,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts13-sts",
            "name": "MTEB STS13",
            "config": "default",
            "split": "test",
            "revision": "7e90230a92c190f1bf69ae9002b8cea547a64cca"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 84.96937598668538,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 85.20181466598035,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 84.51715977112744,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 85.20181466598035,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 84.45150037846719,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 85.12338939049123,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts14-sts",
            "name": "MTEB STS14",
            "config": "default",
            "split": "test",
            "revision": "6031580fec1f6af667f0bd2da0a551cf4f0b2375"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 84.58787775650663,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 80.97859876561874,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 83.38711461294801,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 80.97859876561874,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 83.34934127987394,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 80.9556224835537,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts15-sts",
            "name": "MTEB STS15",
            "config": "default",
            "split": "test",
            "revision": "ae752c7c21bf194d8b67fd573edf7ae58183cbe3"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 88.57387982528677,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 89.22666720704161,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 88.50953296228646,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 89.22666720704161,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 88.45343635855095,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 89.1638631562071,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts16-sts",
            "name": "MTEB STS16",
            "config": "default",
            "split": "test",
            "revision": "4d8694f8f0e0100860b497b999b3dbed754a0513"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.26071496425682,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 86.31740966379304,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 85.85515938268887,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 86.31740966379304,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 85.80077191882177,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 86.27885602957302,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts17-crosslingual-sts",
            "name": "MTEB STS17 (en-en)",
            "config": "en-en",
            "split": "test",
            "revision": "af5e6fb845001ecf41f4c1e033ce921939a2a68d"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 90.41413251495673,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 90.3370719075361,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 90.5785973346113,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 90.3370719075361,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 90.5278703024898,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 90.23870483011629,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts22-crosslingual-sts",
            "name": "MTEB STS22 (en)",
            "config": "en",
            "split": "test",
            "revision": "6d1ba47164174a496b7fa5d3569dae26a6813b80"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 66.1571023517868,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 66.42297916256133,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 67.55835224919745,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 66.42297916256133,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 67.40537247802385,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 66.26259339863576,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/stsbenchmark-sts",
            "name": "MTEB STSBenchmark",
            "config": "default",
            "split": "test",
            "revision": "b0fddb56ed78048fa8b90373c8a3cfc37b684831"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 87.4251695055504,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 88.54881886307972,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 88.54094330250571,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 88.54881886307972,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 88.49069549839685,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 88.49149164694148,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/scidocs-reranking",
            "name": "MTEB SciDocsRR",
            "config": "default",
            "split": "test",
            "revision": "d3c5e1fc0b855ab6097bf1cda04dd73947d7caab"
          },
          "metrics": [
            {
              "type": "map",
              "value": 85.19974508901711,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 95.95137342686361,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "scifact",
            "name": "MTEB SciFact",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 71.825,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/sprintduplicatequestions-pairclassification",
            "name": "MTEB SprintDuplicateQuestions",
            "config": "default",
            "split": "test",
            "revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 99.85346534653465,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 96.2457455868878,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 92.49492900608519,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 93.82716049382715,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 91.2,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 99.85346534653465,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 96.24574558688776,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 92.49492900608519,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 93.82716049382715,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 91.2,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 99.85346534653465,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 96.2457455868878,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 92.49492900608519,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 93.82716049382715,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 91.2,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 99.85643564356435,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 96.24594126679709,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 92.63585576434738,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 94.11764705882352,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 91.2,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 99.85643564356435,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 96.24594126679709,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 92.63585576434738,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/stackexchange-clustering",
            "name": "MTEB StackExchangeClustering",
            "config": "default",
            "split": "test",
            "revision": "6cbc1f7b2bc0622f2e39d2c77fa502909748c259"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 68.41861859721674,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/stackexchange-clustering-p2p",
            "name": "MTEB StackExchangeClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "815ca46b2622cec33ccafc3735d572c266efdb44"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 37.51202861563424,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/stackoverflowdupquestions-reranking",
            "name": "MTEB StackOverflowDupQuestions",
            "config": "default",
            "split": "test",
            "revision": "e185fbe320c72810689fc5848eb6114e1ef5ec69"
          },
          "metrics": [
            {
              "type": "map",
              "value": 52.48207537634766,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 53.36204747050335,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Summarization"
          },
          "dataset": {
            "type": "mteb/summeval",
            "name": "MTEB SummEval",
            "config": "default",
            "split": "test",
            "revision": "cda12ad7615edc362dbf25a00fdd61d3b1eaf93c"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 30.397150340510397,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 30.180928192386,
              "verified": false
            },
            {
              "type": "dot_pearson",
              "value": 30.397148822378796,
              "verified": false
            },
            {
              "type": "dot_spearman",
              "value": 30.180928192386,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "trec-covid",
            "name": "MTEB TRECCOVID",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 81.919,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "webis-touche2020",
            "name": "MTEB Touche2020",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 32.419,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/toxic_conversations_50k",
            "name": "MTEB ToxicConversationsClassification",
            "config": "default",
            "split": "test",
            "revision": "d7c0de2777da35d6aae2200a62c6e0e5af397c4c"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 72.613,
              "verified": false
            },
            {
              "type": "ap",
              "value": 15.696112954573444,
              "verified": false
            },
            {
              "type": "f1",
              "value": 56.30148693392767,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/tweet_sentiment_extraction",
            "name": "MTEB TweetSentimentExtractionClassification",
            "config": "default",
            "split": "test",
            "revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 62.02037351443125,
              "verified": false
            },
            {
              "type": "f1",
              "value": 62.31189055427593,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/twentynewsgroups-clustering",
            "name": "MTEB TwentyNewsgroupsClustering",
            "config": "default",
            "split": "test",
            "revision": "6125ec4e24fa026cec8a478383ee943acfbd5449"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 50.64186455543417,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/twittersemeval2015-pairclassification",
            "name": "MTEB TwitterSemEval2015",
            "config": "default",
            "split": "test",
            "revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 86.27883411813792,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 74.80076733774258,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 68.97989210397255,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 64.42968392120935,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 74.22163588390501,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 86.27883411813792,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 74.80076608107143,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 68.97989210397255,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 64.42968392120935,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 74.22163588390501,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 86.27883411813792,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 74.80076820459502,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 68.97989210397255,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 64.42968392120935,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 74.22163588390501,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 86.23711032961793,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 74.73958348950038,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 68.76052948255115,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 63.207964601769916,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 75.3825857519789,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 86.27883411813792,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 74.80076820459502,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 68.97989210397255,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/twitterurlcorpus-pairclassification",
            "name": "MTEB TwitterURLCorpus",
            "config": "default",
            "split": "test",
            "revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 89.09263787014399,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 86.46378381763645,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 78.67838784176413,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 76.20868812238419,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 81.3135201724669,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 89.09263787014399,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 86.46378353247907,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 78.67838784176413,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 76.20868812238419,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 81.3135201724669,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 89.09263787014399,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 86.46378511891255,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 78.67838784176413,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 76.20868812238419,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 81.3135201724669,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 89.09069740365584,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 86.44864502475154,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 78.67372818141132,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 76.29484953703704,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 81.20572836464429,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 89.09263787014399,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 86.46378511891255,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 78.67838784176413,
              "verified": false
            }
          ]
        }
      ]
    }
  ],
  "config": {
    "tokenizer_config": {
      "cls_token": "[CLS]",
      "mask_token": "[MASK]",
      "pad_token": "[PAD]",
      "sep_token": "[SEP]",
      "unk_token": "[UNK]"
    }
  },
  "cardData": {
    "tags": [
      "mteb"
    ],
    "model-index": [
      {
        "name": "embed-english-v3.0",
        "results": [
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_counterfactual",
              "name": "MTEB AmazonCounterfactualClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "e8379541af4e31359cca9fbcf4b00f2671dba205"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 81.29850746268656,
                "verified": false
              },
              {
                "type": "ap",
                "value": 46.181772245676136,
                "verified": false
              },
              {
                "type": "f1",
                "value": 75.47731234579823,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_polarity",
              "name": "MTEB AmazonPolarityClassification",
              "config": "default",
              "split": "test",
              "revision": "e2d317d38cd51312af73b3d32a06d1a08b442046"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 95.61824999999999,
                "verified": false
              },
              {
                "type": "ap",
                "value": 93.22525741797098,
                "verified": false
              },
              {
                "type": "f1",
                "value": 95.61627312544859,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_reviews_multi",
              "name": "MTEB AmazonReviewsClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "1399c76144fd37290681b995c656ef9b2e06e26d"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 51.72,
                "verified": false
              },
              {
                "type": "f1",
                "value": 50.529480725642465,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "arguana",
              "name": "MTEB ArguAna",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 61.521,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/arxiv-clustering-p2p",
              "name": "MTEB ArxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "a122ad7f3f0291bf49cc6f4d32aa80929df69d5d"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 49.173332266218914,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/arxiv-clustering-s2s",
              "name": "MTEB ArxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "f910caf1a6075f7329cdf8c1a6135696f37dbd53"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 42.1800504937582,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/askubuntudupquestions-reranking",
              "name": "MTEB AskUbuntuDupQuestions",
              "config": "default",
              "split": "test",
              "revision": "2000358ca161889fa9c082cb41daa8dcfb161a54"
            },
            "metrics": [
              {
                "type": "map",
                "value": 61.69942465283367,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 73.8089741898606,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/biosses-sts",
              "name": "MTEB BIOSSES",
              "config": "default",
              "split": "test",
              "revision": "d3fb88f8f02e40887cd149695127462bbcf29b4a"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.1805709775319,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 83.50310749422796,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 83.57134970408762,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 83.50310749422796,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 83.422472116232,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 83.35611619312422,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/banking77",
              "name": "MTEB Banking77Classification",
              "config": "default",
              "split": "test",
              "revision": "0fd18e25b25c072e09e0d92ab615fda904d66300"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 85.52922077922078,
                "verified": false
              },
              {
                "type": "f1",
                "value": 85.48530911742581,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/biorxiv-clustering-p2p",
              "name": "MTEB BiorxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "65b79d1d13f80053f67aca9498d9402c2d9f1f40"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 40.95750155360001,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/biorxiv-clustering-s2s",
              "name": "MTEB BiorxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "258694dd0231531bc1fd9de6ceb52a0853c6d908"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 37.25334765305169,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackAndroidRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 50.037,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackEnglishRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 49.089,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackGamingRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 60.523,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackGisRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 39.293,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackMathematicaRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 30.414,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackPhysicsRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 43.662,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackProgrammersRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 43.667,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 41.53158333333334,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackStatsRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 35.258,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackTexRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 30.866,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackUnixRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 40.643,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackWebmastersRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 40.663,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackWordpressRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 34.264,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "climate-fever",
              "name": "MTEB ClimateFEVER",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 38.433,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "dbpedia-entity",
              "name": "MTEB DBPedia",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 43.36,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/emotion",
              "name": "MTEB EmotionClassification",
              "config": "default",
              "split": "test",
              "revision": "4f58c6b202a23cf9a4da393831edf4f9183cad37"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 51.574999999999996,
                "verified": false
              },
              {
                "type": "f1",
                "value": 46.84362123583929,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "fever",
              "name": "MTEB FEVER",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 88.966,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "fiqa",
              "name": "MTEB FiQA2018",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 42.189,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "hotpotqa",
              "name": "MTEB HotpotQA",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 70.723,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/imdb",
              "name": "MTEB ImdbClassification",
              "config": "default",
              "split": "test",
              "revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 93.56920000000001,
                "verified": false
              },
              {
                "type": "ap",
                "value": 90.56104192134326,
                "verified": false
              },
              {
                "type": "f1",
                "value": 93.56471146876505,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "msmarco",
              "name": "MTEB MSMARCO",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 42.931000000000004,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/mtop_domain",
              "name": "MTEB MTOPDomainClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 94.88372093023256,
                "verified": false
              },
              {
                "type": "f1",
                "value": 94.64417024711646,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/mtop_intent",
              "name": "MTEB MTOPIntentClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 76.52302781577748,
                "verified": false
              },
              {
                "type": "f1",
                "value": 59.52848723786157,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_massive_intent",
              "name": "MTEB MassiveIntentClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 73.84330867518494,
                "verified": false
              },
              {
                "type": "f1",
                "value": 72.18121296285702,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_massive_scenario",
              "name": "MTEB MassiveScenarioClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "7d571f92784cd94a019292a1f45445077d0ef634"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 78.73907195696033,
                "verified": false
              },
              {
                "type": "f1",
                "value": 78.86079300338558,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/medrxiv-clustering-p2p",
              "name": "MTEB MedrxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "e7a26af6f3ae46b30dde8737f02c07b1505bcc73"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 37.40673427491627,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/medrxiv-clustering-s2s",
              "name": "MTEB MedrxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "35191c8c0dca72d8ff3efcd72aa802307d469663"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 33.38936252583581,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/mind_small",
              "name": "MTEB MindSmallReranking",
              "config": "default",
              "split": "test",
              "revision": "3bdac13927fdc888b903db93b2ffdbd90b295a69"
            },
            "metrics": [
              {
                "type": "map",
                "value": 32.67317850167471,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 33.9334102169254,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "nfcorpus",
              "name": "MTEB NFCorpus",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 38.574000000000005,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "nq",
              "name": "MTEB NQ",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 61.556,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "quora",
              "name": "MTEB QuoraRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 88.722,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/reddit-clustering",
              "name": "MTEB RedditClustering",
              "config": "default",
              "split": "test",
              "revision": "24640382cdbf8abc73003fb0fa6d111a705499eb"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 58.45790556534654,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/reddit-clustering-p2p",
              "name": "MTEB RedditClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "282350215ef01743dc01b456c7f5241fa8937f16"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 66.35141658656822,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "scidocs",
              "name": "MTEB SCIDOCS",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 20.314,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sickr-sts",
              "name": "MTEB SICK-R",
              "config": "default",
              "split": "test",
              "revision": "a6ea5a8cab320b040a23452cc28066d9beae2cee"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.49945063881191,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 81.27177640994141,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 82.74613694646263,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 81.2717795980493,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 82.75268512220467,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 81.28362006796547,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts12-sts",
              "name": "MTEB STS12",
              "config": "default",
              "split": "test",
              "revision": "a0d554a64d88156834ff5ae9920b964011b16384"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 83.17562591888526,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 74.37099514810372,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 79.97392043583372,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 74.37103618585903,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 80.00641585184354,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 74.35403985608939,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts13-sts",
              "name": "MTEB STS13",
              "config": "default",
              "split": "test",
              "revision": "7e90230a92c190f1bf69ae9002b8cea547a64cca"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 84.96937598668538,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 85.20181466598035,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 84.51715977112744,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 85.20181466598035,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 84.45150037846719,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 85.12338939049123,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts14-sts",
              "name": "MTEB STS14",
              "config": "default",
              "split": "test",
              "revision": "6031580fec1f6af667f0bd2da0a551cf4f0b2375"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 84.58787775650663,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 80.97859876561874,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 83.38711461294801,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 80.97859876561874,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 83.34934127987394,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 80.9556224835537,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts15-sts",
              "name": "MTEB STS15",
              "config": "default",
              "split": "test",
              "revision": "ae752c7c21bf194d8b67fd573edf7ae58183cbe3"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 88.57387982528677,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 89.22666720704161,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 88.50953296228646,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 89.22666720704161,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 88.45343635855095,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 89.1638631562071,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts16-sts",
              "name": "MTEB STS16",
              "config": "default",
              "split": "test",
              "revision": "4d8694f8f0e0100860b497b999b3dbed754a0513"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.26071496425682,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 86.31740966379304,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 85.85515938268887,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 86.31740966379304,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 85.80077191882177,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 86.27885602957302,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts17-crosslingual-sts",
              "name": "MTEB STS17 (en-en)",
              "config": "en-en",
              "split": "test",
              "revision": "af5e6fb845001ecf41f4c1e033ce921939a2a68d"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 90.41413251495673,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 90.3370719075361,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 90.5785973346113,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 90.3370719075361,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 90.5278703024898,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 90.23870483011629,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts22-crosslingual-sts",
              "name": "MTEB STS22 (en)",
              "config": "en",
              "split": "test",
              "revision": "6d1ba47164174a496b7fa5d3569dae26a6813b80"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 66.1571023517868,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 66.42297916256133,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 67.55835224919745,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 66.42297916256133,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 67.40537247802385,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 66.26259339863576,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/stsbenchmark-sts",
              "name": "MTEB STSBenchmark",
              "config": "default",
              "split": "test",
              "revision": "b0fddb56ed78048fa8b90373c8a3cfc37b684831"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 87.4251695055504,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 88.54881886307972,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 88.54094330250571,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 88.54881886307972,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 88.49069549839685,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 88.49149164694148,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/scidocs-reranking",
              "name": "MTEB SciDocsRR",
              "config": "default",
              "split": "test",
              "revision": "d3c5e1fc0b855ab6097bf1cda04dd73947d7caab"
            },
            "metrics": [
              {
                "type": "map",
                "value": 85.19974508901711,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 95.95137342686361,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "scifact",
              "name": "MTEB SciFact",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 71.825,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/sprintduplicatequestions-pairclassification",
              "name": "MTEB SprintDuplicateQuestions",
              "config": "default",
              "split": "test",
              "revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 99.85346534653465,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 96.2457455868878,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 92.49492900608519,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 93.82716049382715,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 91.2,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 99.85346534653465,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 96.24574558688776,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 92.49492900608519,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 93.82716049382715,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 91.2,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 99.85346534653465,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 96.2457455868878,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 92.49492900608519,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 93.82716049382715,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 91.2,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 99.85643564356435,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 96.24594126679709,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 92.63585576434738,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 94.11764705882352,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 91.2,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 99.85643564356435,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 96.24594126679709,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 92.63585576434738,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/stackexchange-clustering",
              "name": "MTEB StackExchangeClustering",
              "config": "default",
              "split": "test",
              "revision": "6cbc1f7b2bc0622f2e39d2c77fa502909748c259"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 68.41861859721674,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/stackexchange-clustering-p2p",
              "name": "MTEB StackExchangeClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "815ca46b2622cec33ccafc3735d572c266efdb44"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 37.51202861563424,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/stackoverflowdupquestions-reranking",
              "name": "MTEB StackOverflowDupQuestions",
              "config": "default",
              "split": "test",
              "revision": "e185fbe320c72810689fc5848eb6114e1ef5ec69"
            },
            "metrics": [
              {
                "type": "map",
                "value": 52.48207537634766,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 53.36204747050335,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Summarization"
            },
            "dataset": {
              "type": "mteb/summeval",
              "name": "MTEB SummEval",
              "config": "default",
              "split": "test",
              "revision": "cda12ad7615edc362dbf25a00fdd61d3b1eaf93c"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 30.397150340510397,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 30.180928192386,
                "verified": false
              },
              {
                "type": "dot_pearson",
                "value": 30.397148822378796,
                "verified": false
              },
              {
                "type": "dot_spearman",
                "value": 30.180928192386,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "trec-covid",
              "name": "MTEB TRECCOVID",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 81.919,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "webis-touche2020",
              "name": "MTEB Touche2020",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 32.419,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/toxic_conversations_50k",
              "name": "MTEB ToxicConversationsClassification",
              "config": "default",
              "split": "test",
              "revision": "d7c0de2777da35d6aae2200a62c6e0e5af397c4c"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 72.613,
                "verified": false
              },
              {
                "type": "ap",
                "value": 15.696112954573444,
                "verified": false
              },
              {
                "type": "f1",
                "value": 56.30148693392767,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/tweet_sentiment_extraction",
              "name": "MTEB TweetSentimentExtractionClassification",
              "config": "default",
              "split": "test",
              "revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 62.02037351443125,
                "verified": false
              },
              {
                "type": "f1",
                "value": 62.31189055427593,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/twentynewsgroups-clustering",
              "name": "MTEB TwentyNewsgroupsClustering",
              "config": "default",
              "split": "test",
              "revision": "6125ec4e24fa026cec8a478383ee943acfbd5449"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 50.64186455543417,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/twittersemeval2015-pairclassification",
              "name": "MTEB TwitterSemEval2015",
              "config": "default",
              "split": "test",
              "revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 86.27883411813792,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 74.80076733774258,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 68.97989210397255,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 64.42968392120935,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 74.22163588390501,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 86.27883411813792,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 74.80076608107143,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 68.97989210397255,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 64.42968392120935,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 74.22163588390501,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 86.27883411813792,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 74.80076820459502,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 68.97989210397255,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 64.42968392120935,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 74.22163588390501,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 86.23711032961793,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 74.73958348950038,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 68.76052948255115,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 63.207964601769916,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 75.3825857519789,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 86.27883411813792,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 74.80076820459502,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 68.97989210397255,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/twitterurlcorpus-pairclassification",
              "name": "MTEB TwitterURLCorpus",
              "config": "default",
              "split": "test",
              "revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 89.09263787014399,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 86.46378381763645,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 78.67838784176413,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 76.20868812238419,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 81.3135201724669,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 89.09263787014399,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 86.46378353247907,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 78.67838784176413,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 76.20868812238419,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 81.3135201724669,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 89.09263787014399,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 86.46378511891255,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 78.67838784176413,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 76.20868812238419,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 81.3135201724669,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 89.09069740365584,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 86.44864502475154,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 78.67372818141132,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 76.29484953703704,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 81.20572836464429,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 89.09263787014399,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 86.46378511891255,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 78.67838784176413,
                "verified": false
              }
            ]
          }
        ]
      }
    ]
  },
  "transformersInfo": {
    "auto_model": "AutoModel"
  },
  "siblings": [
    {
      "rfilename": ".gitattributes"
    },
    {
      "rfilename": "README.md"
    },
    {
      "rfilename": "added_tokens.json"
    },
    {
      "rfilename": "config.json"
    },
    {
      "rfilename": "special_tokens_map.json"
    },
    {
      "rfilename": "tokenizer.json"
    },
    {
      "rfilename": "tokenizer_config.json"
    },
    {
      "rfilename": "vocab.txt"
    }
  ],
  "spaces": [
    "mteb/leaderboard",
    "mteb/leaderboard_legacy",
    "ManFrag/translatetopic",
    "SmileXing/leaderboard",
    "sq66/leaderboard_legacy",
    "q275343119/leaderboard",
    "AyushM6/leaderboard",
    "shiwan7788/leaderboard",
    "Chengyanci/11",
    "yanciyuyu/1",
    "n8n-1/8",
    "reader-1/1"
  ],
  "createdAt": "2023-11-02T12:24:52.000Z",
  "usedStorage": 0
}