{
  "_id": "6543715dff2ffbf4176a91a0",
  "id": "Cohere/Cohere-embed-multilingual-v3.0",
  "private": false,
  "library_name": "transformers",
  "tags": [
    "transformers",
    "mteb",
    "model-index",
    "endpoints_compatible",
    "region:us"
  ],
  "downloads": 534,
  "likes": 101,
  "modelId": "Cohere/Cohere-embed-multilingual-v3.0",
  "author": "Cohere",
  "sha": "0909d6e3b0912a0dc72f15cc70cab568495d0df0",
  "lastModified": "2023-11-07T12:59:44.000Z",
  "gated": false,
  "disabled": false,
  "model-index": [
    {
      "name": "embed-multilingual-v3.0",
      "results": [
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_counterfactual",
            "name": "MTEB AmazonCounterfactualClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "e8379541af4e31359cca9fbcf4b00f2671dba205"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 77.85074626865672,
              "verified": false
            },
            {
              "type": "ap",
              "value": 41.53151744002314,
              "verified": false
            },
            {
              "type": "f1",
              "value": 71.94656880817726,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_polarity",
            "name": "MTEB AmazonPolarityClassification",
            "config": "default",
            "split": "test",
            "revision": "e2d317d38cd51312af73b3d32a06d1a08b442046"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 95.600375,
              "verified": false
            },
            {
              "type": "ap",
              "value": 93.57882128753579,
              "verified": false
            },
            {
              "type": "f1",
              "value": 95.59945484944305,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_reviews_multi",
            "name": "MTEB AmazonReviewsClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "1399c76144fd37290681b995c656ef9b2e06e26d"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 49.794,
              "verified": false
            },
            {
              "type": "f1",
              "value": 48.740439663130985,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "arguana",
            "name": "MTEB ArguAna",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 55.105000000000004,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/arxiv-clustering-p2p",
            "name": "MTEB ArxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "a122ad7f3f0291bf49cc6f4d32aa80929df69d5d"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 48.15653426568874,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/arxiv-clustering-s2s",
            "name": "MTEB ArxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "f910caf1a6075f7329cdf8c1a6135696f37dbd53"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 40.78876256237919,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/askubuntudupquestions-reranking",
            "name": "MTEB AskUbuntuDupQuestions",
            "config": "default",
            "split": "test",
            "revision": "2000358ca161889fa9c082cb41daa8dcfb161a54"
          },
          "metrics": [
            {
              "type": "map",
              "value": 62.12873500780318,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 75.87037769863255,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/biosses-sts",
            "name": "MTEB BIOSSES",
            "config": "default",
            "split": "test",
            "revision": "d3fb88f8f02e40887cd149695127462bbcf29b4a"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 86.01183720167818,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 85.00916590717613,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 84.072733561361,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 85.00916590717613,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 83.89233507343208,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 84.87482549674115,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/banking77",
            "name": "MTEB Banking77Classification",
            "config": "default",
            "split": "test",
            "revision": "0fd18e25b25c072e09e0d92ab615fda904d66300"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 86.09415584415584,
              "verified": false
            },
            {
              "type": "f1",
              "value": 86.05173549773973,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/biorxiv-clustering-p2p",
            "name": "MTEB BiorxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "65b79d1d13f80053f67aca9498d9402c2d9f1f40"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 40.49773000165541,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/biorxiv-clustering-s2s",
            "name": "MTEB BiorxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "258694dd0231531bc1fd9de6ceb52a0853c6d908"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 36.909633073998876,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackAndroidRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 49.481,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackEnglishRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 47.449999999999996,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackGamingRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 59.227,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackGisRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 37.729,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackMathematicaRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 29.673,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackPhysicsRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 44.278,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackProgrammersRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 43.218,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 40.63741666666667,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackStatsRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 33.341,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackTexRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 29.093999999999998,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackUnixRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 40.801,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackWebmastersRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 40.114,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackWordpressRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 33.243,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "climate-fever",
            "name": "MTEB ClimateFEVER",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 29.958000000000002,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "dbpedia-entity",
            "name": "MTEB DBPedia",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 41.004000000000005,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/emotion",
            "name": "MTEB EmotionClassification",
            "config": "default",
            "split": "test",
            "revision": "4f58c6b202a23cf9a4da393831edf4f9183cad37"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 48.150000000000006,
              "verified": false
            },
            {
              "type": "f1",
              "value": 43.69803436468346,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "fever",
            "name": "MTEB FEVER",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 88.532,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "fiqa",
            "name": "MTEB FiQA2018",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 44.105,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "hotpotqa",
            "name": "MTEB HotpotQA",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 70.612,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/imdb",
            "name": "MTEB ImdbClassification",
            "config": "default",
            "split": "test",
            "revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 93.9672,
              "verified": false
            },
            {
              "type": "ap",
              "value": 90.72947025321227,
              "verified": false
            },
            {
              "type": "f1",
              "value": 93.96271599852622,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "msmarco",
            "name": "MTEB MSMARCO",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 43.447,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/mtop_domain",
            "name": "MTEB MTOPDomainClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 94.92476060191517,
              "verified": false
            },
            {
              "type": "f1",
              "value": 94.69383758972194,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/mtop_intent",
            "name": "MTEB MTOPIntentClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 78.8873689010488,
              "verified": false
            },
            {
              "type": "f1",
              "value": 62.537485052253885,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_massive_intent",
            "name": "MTEB MassiveIntentClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 74.51244115669132,
              "verified": false
            },
            {
              "type": "f1",
              "value": 72.40074466830153,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_massive_scenario",
            "name": "MTEB MassiveScenarioClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "7d571f92784cd94a019292a1f45445077d0ef634"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 79.00470746469401,
              "verified": false
            },
            {
              "type": "f1",
              "value": 79.03758200183096,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/medrxiv-clustering-p2p",
            "name": "MTEB MedrxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "e7a26af6f3ae46b30dde8737f02c07b1505bcc73"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 36.183215937303736,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/medrxiv-clustering-s2s",
            "name": "MTEB MedrxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "35191c8c0dca72d8ff3efcd72aa802307d469663"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 33.443759055792135,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/mind_small",
            "name": "MTEB MindSmallReranking",
            "config": "default",
            "split": "test",
            "revision": "3bdac13927fdc888b903db93b2ffdbd90b295a69"
          },
          "metrics": [
            {
              "type": "map",
              "value": 32.58713095176127,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 33.7326038566206,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "nfcorpus",
            "name": "MTEB NFCorpus",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 36.417,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "nq",
            "name": "MTEB NQ",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 63.415,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "quora",
            "name": "MTEB QuoraRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 88.924,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/reddit-clustering",
            "name": "MTEB RedditClustering",
            "config": "default",
            "split": "test",
            "revision": "24640382cdbf8abc73003fb0fa6d111a705499eb"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 58.10997801688676,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/reddit-clustering-p2p",
            "name": "MTEB RedditClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "282350215ef01743dc01b456c7f5241fa8937f16"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 65.02444843766075,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "scidocs",
            "name": "MTEB SCIDOCS",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 19.339000000000002,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sickr-sts",
            "name": "MTEB SICK-R",
            "config": "default",
            "split": "test",
            "revision": "a6ea5a8cab320b040a23452cc28066d9beae2cee"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 86.61540076033945,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 82.1820253476181,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 83.73901215845989,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 82.182021064594,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 83.76685139192031,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 82.14074705306663,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts12-sts",
            "name": "MTEB STS12",
            "config": "default",
            "split": "test",
            "revision": "a0d554a64d88156834ff5ae9920b964011b16384"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.62241109228789,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 77.62042143066208,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 82.77237785274072,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 77.62042142290566,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 82.70945589621266,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 77.57245632826351,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts13-sts",
            "name": "MTEB STS13",
            "config": "default",
            "split": "test",
            "revision": "7e90230a92c190f1bf69ae9002b8cea547a64cca"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 84.8307075352031,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 85.15620774806095,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 84.21956724564915,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 85.15620774806095,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 84.0677597021641,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 85.02572172855729,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts14-sts",
            "name": "MTEB STS14",
            "config": "default",
            "split": "test",
            "revision": "6031580fec1f6af667f0bd2da0a551cf4f0b2375"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 83.33749463516592,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 80.01967438481185,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 82.16884494022196,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 80.01967218194336,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 81.94431512413773,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 79.81636247503731,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts15-sts",
            "name": "MTEB STS15",
            "config": "default",
            "split": "test",
            "revision": "ae752c7c21bf194d8b67fd573edf7ae58183cbe3"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 88.2070761097028,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 88.92297656560552,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 87.95961374550303,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 88.92298798854765,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 87.85515971478168,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 88.8100644762342,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts16-sts",
            "name": "MTEB STS16",
            "config": "default",
            "split": "test",
            "revision": "4d8694f8f0e0100860b497b999b3dbed754a0513"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.48103354546488,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 86.91850928862898,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 86.06766986527145,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 86.91850928862898,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 86.02705585360717,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 86.86666545434721,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts17-crosslingual-sts",
            "name": "MTEB STS17 (en-en)",
            "config": "en-en",
            "split": "test",
            "revision": "af5e6fb845001ecf41f4c1e033ce921939a2a68d"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 90.30267248880148,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 90.08752166657892,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 90.4697525265135,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 90.08752166657892,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 90.57174978064741,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 90.212834942229,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts22-crosslingual-sts",
            "name": "MTEB STS22 (en)",
            "config": "en",
            "split": "test",
            "revision": "6d1ba47164174a496b7fa5d3569dae26a6813b80"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 67.10616236380835,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 66.81483164137016,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 68.48505128040803,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 66.81483164137016,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 68.46133268524885,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 66.83684227990202,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/stsbenchmark-sts",
            "name": "MTEB STSBenchmark",
            "config": "default",
            "split": "test",
            "revision": "b0fddb56ed78048fa8b90373c8a3cfc37b684831"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 87.12768629069949,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 88.78683817318573,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 88.47603251297261,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 88.78683817318573,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 88.46483630890225,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 88.76593424921617,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/scidocs-reranking",
            "name": "MTEB SciDocsRR",
            "config": "default",
            "split": "test",
            "revision": "d3c5e1fc0b855ab6097bf1cda04dd73947d7caab"
          },
          "metrics": [
            {
              "type": "map",
              "value": 84.30886658431281,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 95.5964251797585,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "scifact",
            "name": "MTEB SciFact",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 70.04599999999999,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/sprintduplicatequestions-pairclassification",
            "name": "MTEB SprintDuplicateQuestions",
            "config": "default",
            "split": "test",
            "revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 99.87524752475248,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 96.79160651306724,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 93.57798165137615,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 95.42619542619542,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 91.8,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 99.87524752475248,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 96.79160651306724,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 93.57798165137615,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 95.42619542619542,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 91.8,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 99.87524752475248,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 96.79160651306724,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 93.57798165137615,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 95.42619542619542,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 91.8,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 99.87326732673267,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 96.7574606340297,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 93.45603271983639,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 95.60669456066945,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 91.4,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 99.87524752475248,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 96.79160651306724,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 93.57798165137615,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/stackexchange-clustering",
            "name": "MTEB StackExchangeClustering",
            "config": "default",
            "split": "test",
            "revision": "6cbc1f7b2bc0622f2e39d2c77fa502909748c259"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 68.12288811917144,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/stackexchange-clustering-p2p",
            "name": "MTEB StackExchangeClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "815ca46b2622cec33ccafc3735d572c266efdb44"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 35.22267280169542,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/stackoverflowdupquestions-reranking",
            "name": "MTEB StackOverflowDupQuestions",
            "config": "default",
            "split": "test",
            "revision": "e185fbe320c72810689fc5848eb6114e1ef5ec69"
          },
          "metrics": [
            {
              "type": "map",
              "value": 52.39780995606098,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 53.26826563958916,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Summarization"
          },
          "dataset": {
            "type": "mteb/summeval",
            "name": "MTEB SummEval",
            "config": "default",
            "split": "test",
            "revision": "cda12ad7615edc362dbf25a00fdd61d3b1eaf93c"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 31.15118979569649,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 30.99428921914572,
              "verified": false
            },
            {
              "type": "dot_pearson",
              "value": 31.151189338601924,
              "verified": false
            },
            {
              "type": "dot_spearman",
              "value": 30.99428921914572,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "trec-covid",
            "name": "MTEB TRECCOVID",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 83.372,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "webis-touche2020",
            "name": "MTEB Touche2020",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 32.698,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/toxic_conversations_50k",
            "name": "MTEB ToxicConversationsClassification",
            "config": "default",
            "split": "test",
            "revision": "d7c0de2777da35d6aae2200a62c6e0e5af397c4c"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 71.1998,
              "verified": false
            },
            {
              "type": "ap",
              "value": 14.646205259325157,
              "verified": false
            },
            {
              "type": "f1",
              "value": 54.96172518137252,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/tweet_sentiment_extraction",
            "name": "MTEB TweetSentimentExtractionClassification",
            "config": "default",
            "split": "test",
            "revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 62.176004527447645,
              "verified": false
            },
            {
              "type": "f1",
              "value": 62.48549068096645,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/twentynewsgroups-clustering",
            "name": "MTEB TwentyNewsgroupsClustering",
            "config": "default",
            "split": "test",
            "revision": "6125ec4e24fa026cec8a478383ee943acfbd5449"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 50.13767789739772,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/twittersemeval2015-pairclassification",
            "name": "MTEB TwitterSemEval2015",
            "config": "default",
            "split": "test",
            "revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 86.38016331882935,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 75.1635976260804,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 69.29936305732484,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 66.99507389162561,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 71.76781002638522,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 86.38016331882935,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 75.16359359202374,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 69.29936305732484,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 66.99507389162561,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 71.76781002638522,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 86.38016331882935,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 75.16360246558416,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 69.29936305732484,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 66.99507389162561,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 71.76781002638522,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 86.27883411813792,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 75.02872038741897,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 69.29256284011403,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 68.07535641547861,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 70.55408970976254,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 86.38016331882935,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 75.16360246558416,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 69.29936305732484,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/twitterurlcorpus-pairclassification",
            "name": "MTEB TwitterURLCorpus",
            "config": "default",
            "split": "test",
            "revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 89.39729110878255,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 86.48560260020555,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 79.35060602690982,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 76.50632549496105,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 82.41453649522637,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 89.39729110878255,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 86.48559829915334,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 79.35060602690982,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 76.50632549496105,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 82.41453649522637,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 89.39729110878255,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 86.48559993122497,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 79.35060602690982,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 76.50632549496105,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 82.41453649522637,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 89.36042224550782,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 86.47238558562499,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 79.24500641378047,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 75.61726236273344,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 83.23837388358484,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 89.39729110878255,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 86.48560260020555,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 79.35060602690982,
              "verified": false
            }
          ]
        }
      ]
    }
  ],
  "config": {
    "tokenizer_config": {
      "bos_token": "<s>",
      "cls_token": "<s>",
      "eos_token": "</s>",
      "mask_token": {
        "__type": "AddedToken",
        "content": "<mask>",
        "lstrip": true,
        "normalized": true,
        "rstrip": false,
        "single_word": false
      },
      "pad_token": "<pad>",
      "sep_token": "</s>",
      "unk_token": "<unk>"
    }
  },
  "cardData": {
    "tags": [
      "mteb"
    ],
    "model-index": [
      {
        "name": "embed-multilingual-v3.0",
        "results": [
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_counterfactual",
              "name": "MTEB AmazonCounterfactualClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "e8379541af4e31359cca9fbcf4b00f2671dba205"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 77.85074626865672,
                "verified": false
              },
              {
                "type": "ap",
                "value": 41.53151744002314,
                "verified": false
              },
              {
                "type": "f1",
                "value": 71.94656880817726,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_polarity",
              "name": "MTEB AmazonPolarityClassification",
              "config": "default",
              "split": "test",
              "revision": "e2d317d38cd51312af73b3d32a06d1a08b442046"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 95.600375,
                "verified": false
              },
              {
                "type": "ap",
                "value": 93.57882128753579,
                "verified": false
              },
              {
                "type": "f1",
                "value": 95.59945484944305,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_reviews_multi",
              "name": "MTEB AmazonReviewsClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "1399c76144fd37290681b995c656ef9b2e06e26d"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 49.794,
                "verified": false
              },
              {
                "type": "f1",
                "value": 48.740439663130985,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "arguana",
              "name": "MTEB ArguAna",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 55.105000000000004,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/arxiv-clustering-p2p",
              "name": "MTEB ArxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "a122ad7f3f0291bf49cc6f4d32aa80929df69d5d"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 48.15653426568874,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/arxiv-clustering-s2s",
              "name": "MTEB ArxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "f910caf1a6075f7329cdf8c1a6135696f37dbd53"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 40.78876256237919,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/askubuntudupquestions-reranking",
              "name": "MTEB AskUbuntuDupQuestions",
              "config": "default",
              "split": "test",
              "revision": "2000358ca161889fa9c082cb41daa8dcfb161a54"
            },
            "metrics": [
              {
                "type": "map",
                "value": 62.12873500780318,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 75.87037769863255,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/biosses-sts",
              "name": "MTEB BIOSSES",
              "config": "default",
              "split": "test",
              "revision": "d3fb88f8f02e40887cd149695127462bbcf29b4a"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 86.01183720167818,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 85.00916590717613,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 84.072733561361,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 85.00916590717613,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 83.89233507343208,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 84.87482549674115,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/banking77",
              "name": "MTEB Banking77Classification",
              "config": "default",
              "split": "test",
              "revision": "0fd18e25b25c072e09e0d92ab615fda904d66300"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 86.09415584415584,
                "verified": false
              },
              {
                "type": "f1",
                "value": 86.05173549773973,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/biorxiv-clustering-p2p",
              "name": "MTEB BiorxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "65b79d1d13f80053f67aca9498d9402c2d9f1f40"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 40.49773000165541,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/biorxiv-clustering-s2s",
              "name": "MTEB BiorxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "258694dd0231531bc1fd9de6ceb52a0853c6d908"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 36.909633073998876,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackAndroidRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 49.481,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackEnglishRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 47.449999999999996,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackGamingRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 59.227,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackGisRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 37.729,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackMathematicaRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 29.673,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackPhysicsRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 44.278,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackProgrammersRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 43.218,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 40.63741666666667,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackStatsRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 33.341,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackTexRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 29.093999999999998,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackUnixRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 40.801,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackWebmastersRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 40.114,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackWordpressRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 33.243,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "climate-fever",
              "name": "MTEB ClimateFEVER",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 29.958000000000002,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "dbpedia-entity",
              "name": "MTEB DBPedia",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 41.004000000000005,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/emotion",
              "name": "MTEB EmotionClassification",
              "config": "default",
              "split": "test",
              "revision": "4f58c6b202a23cf9a4da393831edf4f9183cad37"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 48.150000000000006,
                "verified": false
              },
              {
                "type": "f1",
                "value": 43.69803436468346,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "fever",
              "name": "MTEB FEVER",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 88.532,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "fiqa",
              "name": "MTEB FiQA2018",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 44.105,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "hotpotqa",
              "name": "MTEB HotpotQA",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 70.612,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/imdb",
              "name": "MTEB ImdbClassification",
              "config": "default",
              "split": "test",
              "revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 93.9672,
                "verified": false
              },
              {
                "type": "ap",
                "value": 90.72947025321227,
                "verified": false
              },
              {
                "type": "f1",
                "value": 93.96271599852622,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "msmarco",
              "name": "MTEB MSMARCO",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 43.447,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/mtop_domain",
              "name": "MTEB MTOPDomainClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 94.92476060191517,
                "verified": false
              },
              {
                "type": "f1",
                "value": 94.69383758972194,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/mtop_intent",
              "name": "MTEB MTOPIntentClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 78.8873689010488,
                "verified": false
              },
              {
                "type": "f1",
                "value": 62.537485052253885,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_massive_intent",
              "name": "MTEB MassiveIntentClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 74.51244115669132,
                "verified": false
              },
              {
                "type": "f1",
                "value": 72.40074466830153,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_massive_scenario",
              "name": "MTEB MassiveScenarioClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "7d571f92784cd94a019292a1f45445077d0ef634"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 79.00470746469401,
                "verified": false
              },
              {
                "type": "f1",
                "value": 79.03758200183096,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/medrxiv-clustering-p2p",
              "name": "MTEB MedrxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "e7a26af6f3ae46b30dde8737f02c07b1505bcc73"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 36.183215937303736,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/medrxiv-clustering-s2s",
              "name": "MTEB MedrxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "35191c8c0dca72d8ff3efcd72aa802307d469663"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 33.443759055792135,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/mind_small",
              "name": "MTEB MindSmallReranking",
              "config": "default",
              "split": "test",
              "revision": "3bdac13927fdc888b903db93b2ffdbd90b295a69"
            },
            "metrics": [
              {
                "type": "map",
                "value": 32.58713095176127,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 33.7326038566206,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "nfcorpus",
              "name": "MTEB NFCorpus",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 36.417,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "nq",
              "name": "MTEB NQ",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 63.415,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "quora",
              "name": "MTEB QuoraRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 88.924,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/reddit-clustering",
              "name": "MTEB RedditClustering",
              "config": "default",
              "split": "test",
              "revision": "24640382cdbf8abc73003fb0fa6d111a705499eb"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 58.10997801688676,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/reddit-clustering-p2p",
              "name": "MTEB RedditClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "282350215ef01743dc01b456c7f5241fa8937f16"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 65.02444843766075,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "scidocs",
              "name": "MTEB SCIDOCS",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 19.339000000000002,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sickr-sts",
              "name": "MTEB SICK-R",
              "config": "default",
              "split": "test",
              "revision": "a6ea5a8cab320b040a23452cc28066d9beae2cee"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 86.61540076033945,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 82.1820253476181,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 83.73901215845989,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 82.182021064594,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 83.76685139192031,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 82.14074705306663,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts12-sts",
              "name": "MTEB STS12",
              "config": "default",
              "split": "test",
              "revision": "a0d554a64d88156834ff5ae9920b964011b16384"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.62241109228789,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 77.62042143066208,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 82.77237785274072,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 77.62042142290566,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 82.70945589621266,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 77.57245632826351,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts13-sts",
              "name": "MTEB STS13",
              "config": "default",
              "split": "test",
              "revision": "7e90230a92c190f1bf69ae9002b8cea547a64cca"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 84.8307075352031,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 85.15620774806095,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 84.21956724564915,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 85.15620774806095,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 84.0677597021641,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 85.02572172855729,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts14-sts",
              "name": "MTEB STS14",
              "config": "default",
              "split": "test",
              "revision": "6031580fec1f6af667f0bd2da0a551cf4f0b2375"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 83.33749463516592,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 80.01967438481185,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 82.16884494022196,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 80.01967218194336,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 81.94431512413773,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 79.81636247503731,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts15-sts",
              "name": "MTEB STS15",
              "config": "default",
              "split": "test",
              "revision": "ae752c7c21bf194d8b67fd573edf7ae58183cbe3"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 88.2070761097028,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 88.92297656560552,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 87.95961374550303,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 88.92298798854765,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 87.85515971478168,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 88.8100644762342,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts16-sts",
              "name": "MTEB STS16",
              "config": "default",
              "split": "test",
              "revision": "4d8694f8f0e0100860b497b999b3dbed754a0513"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.48103354546488,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 86.91850928862898,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 86.06766986527145,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 86.91850928862898,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 86.02705585360717,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 86.86666545434721,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts17-crosslingual-sts",
              "name": "MTEB STS17 (en-en)",
              "config": "en-en",
              "split": "test",
              "revision": "af5e6fb845001ecf41f4c1e033ce921939a2a68d"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 90.30267248880148,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 90.08752166657892,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 90.4697525265135,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 90.08752166657892,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 90.57174978064741,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 90.212834942229,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts22-crosslingual-sts",
              "name": "MTEB STS22 (en)",
              "config": "en",
              "split": "test",
              "revision": "6d1ba47164174a496b7fa5d3569dae26a6813b80"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 67.10616236380835,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 66.81483164137016,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 68.48505128040803,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 66.81483164137016,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 68.46133268524885,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 66.83684227990202,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/stsbenchmark-sts",
              "name": "MTEB STSBenchmark",
              "config": "default",
              "split": "test",
              "revision": "b0fddb56ed78048fa8b90373c8a3cfc37b684831"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 87.12768629069949,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 88.78683817318573,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 88.47603251297261,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 88.78683817318573,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 88.46483630890225,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 88.76593424921617,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/scidocs-reranking",
              "name": "MTEB SciDocsRR",
              "config": "default",
              "split": "test",
              "revision": "d3c5e1fc0b855ab6097bf1cda04dd73947d7caab"
            },
            "metrics": [
              {
                "type": "map",
                "value": 84.30886658431281,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 95.5964251797585,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "scifact",
              "name": "MTEB SciFact",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 70.04599999999999,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/sprintduplicatequestions-pairclassification",
              "name": "MTEB SprintDuplicateQuestions",
              "config": "default",
              "split": "test",
              "revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 99.87524752475248,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 96.79160651306724,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 93.57798165137615,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 95.42619542619542,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 91.8,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 99.87524752475248,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 96.79160651306724,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 93.57798165137615,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 95.42619542619542,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 91.8,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 99.87524752475248,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 96.79160651306724,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 93.57798165137615,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 95.42619542619542,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 91.8,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 99.87326732673267,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 96.7574606340297,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 93.45603271983639,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 95.60669456066945,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 91.4,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 99.87524752475248,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 96.79160651306724,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 93.57798165137615,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/stackexchange-clustering",
              "name": "MTEB StackExchangeClustering",
              "config": "default",
              "split": "test",
              "revision": "6cbc1f7b2bc0622f2e39d2c77fa502909748c259"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 68.12288811917144,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/stackexchange-clustering-p2p",
              "name": "MTEB StackExchangeClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "815ca46b2622cec33ccafc3735d572c266efdb44"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 35.22267280169542,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/stackoverflowdupquestions-reranking",
              "name": "MTEB StackOverflowDupQuestions",
              "config": "default",
              "split": "test",
              "revision": "e185fbe320c72810689fc5848eb6114e1ef5ec69"
            },
            "metrics": [
              {
                "type": "map",
                "value": 52.39780995606098,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 53.26826563958916,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Summarization"
            },
            "dataset": {
              "type": "mteb/summeval",
              "name": "MTEB SummEval",
              "config": "default",
              "split": "test",
              "revision": "cda12ad7615edc362dbf25a00fdd61d3b1eaf93c"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 31.15118979569649,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 30.99428921914572,
                "verified": false
              },
              {
                "type": "dot_pearson",
                "value": 31.151189338601924,
                "verified": false
              },
              {
                "type": "dot_spearman",
                "value": 30.99428921914572,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "trec-covid",
              "name": "MTEB TRECCOVID",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 83.372,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "webis-touche2020",
              "name": "MTEB Touche2020",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 32.698,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/toxic_conversations_50k",
              "name": "MTEB ToxicConversationsClassification",
              "config": "default",
              "split": "test",
              "revision": "d7c0de2777da35d6aae2200a62c6e0e5af397c4c"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 71.1998,
                "verified": false
              },
              {
                "type": "ap",
                "value": 14.646205259325157,
                "verified": false
              },
              {
                "type": "f1",
                "value": 54.96172518137252,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/tweet_sentiment_extraction",
              "name": "MTEB TweetSentimentExtractionClassification",
              "config": "default",
              "split": "test",
              "revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 62.176004527447645,
                "verified": false
              },
              {
                "type": "f1",
                "value": 62.48549068096645,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/twentynewsgroups-clustering",
              "name": "MTEB TwentyNewsgroupsClustering",
              "config": "default",
              "split": "test",
              "revision": "6125ec4e24fa026cec8a478383ee943acfbd5449"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 50.13767789739772,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/twittersemeval2015-pairclassification",
              "name": "MTEB TwitterSemEval2015",
              "config": "default",
              "split": "test",
              "revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 86.38016331882935,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 75.1635976260804,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 69.29936305732484,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 66.99507389162561,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 71.76781002638522,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 86.38016331882935,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 75.16359359202374,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 69.29936305732484,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 66.99507389162561,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 71.76781002638522,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 86.38016331882935,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 75.16360246558416,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 69.29936305732484,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 66.99507389162561,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 71.76781002638522,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 86.27883411813792,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 75.02872038741897,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 69.29256284011403,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 68.07535641547861,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 70.55408970976254,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 86.38016331882935,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 75.16360246558416,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 69.29936305732484,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/twitterurlcorpus-pairclassification",
              "name": "MTEB TwitterURLCorpus",
              "config": "default",
              "split": "test",
              "revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 89.39729110878255,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 86.48560260020555,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 79.35060602690982,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 76.50632549496105,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 82.41453649522637,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 89.39729110878255,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 86.48559829915334,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 79.35060602690982,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 76.50632549496105,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 82.41453649522637,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 89.39729110878255,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 86.48559993122497,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 79.35060602690982,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 76.50632549496105,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 82.41453649522637,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 89.36042224550782,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 86.47238558562499,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 79.24500641378047,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 75.61726236273344,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 83.23837388358484,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 89.39729110878255,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 86.48560260020555,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 79.35060602690982,
                "verified": false
              }
            ]
          }
        ]
      }
    ]
  },
  "transformersInfo": {
    "auto_model": "AutoModel"
  },
  "siblings": [
    {
      "rfilename": ".gitattributes"
    },
    {
      "rfilename": "README.md"
    },
    {
      "rfilename": "config.json"
    },
    {
      "rfilename": "sentencepiece.bpe.model"
    },
    {
      "rfilename": "special_tokens_map.json"
    },
    {
      "rfilename": "tokenizer.json"
    },
    {
      "rfilename": "tokenizer_config.json"
    }
  ],
  "spaces": [
    "mteb/leaderboard",
    "mteb/leaderboard_legacy",
    "5w4n/burmese-tokenizers",
    "SmileXing/leaderboard",
    "sq66/leaderboard_legacy",
    "q275343119/leaderboard",
    "AyushM6/leaderboard",
    "shiwan7788/leaderboard",
    "Chengyanci/11",
    "yanciyuyu/1",
    "n8n-1/8",
    "reader-1/1"
  ],
  "createdAt": "2023-11-02T09:52:29.000Z",
  "usedStorage": 22151711
}