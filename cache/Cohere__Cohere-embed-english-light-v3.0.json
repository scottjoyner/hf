{
  "_id": "6543747917b13c52d7128eb7",
  "id": "Cohere/Cohere-embed-english-light-v3.0",
  "private": false,
  "library_name": "transformers",
  "tags": [
    "transformers",
    "mteb",
    "model-index",
    "endpoints_compatible",
    "region:us"
  ],
  "downloads": 4,
  "likes": 1,
  "modelId": "Cohere/Cohere-embed-english-light-v3.0",
  "author": "Cohere",
  "sha": "af99964c9f9b5356e7c690bec577743df457fcb2",
  "lastModified": "2023-11-02T10:09:41.000Z",
  "gated": false,
  "disabled": false,
  "model-index": [
    {
      "name": "embed-english-light-v3.0",
      "results": [
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_counterfactual",
            "name": "MTEB AmazonCounterfactualClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "e8379541af4e31359cca9fbcf4b00f2671dba205"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 78.62686567164178,
              "verified": false
            },
            {
              "type": "ap",
              "value": 43.50072127690769,
              "verified": false
            },
            {
              "type": "f1",
              "value": 73.12414870629323,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_polarity",
            "name": "MTEB AmazonPolarityClassification",
            "config": "default",
            "split": "test",
            "revision": "e2d317d38cd51312af73b3d32a06d1a08b442046"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 94.795,
              "verified": false
            },
            {
              "type": "ap",
              "value": 92.14178233328848,
              "verified": false
            },
            {
              "type": "f1",
              "value": 94.79269356571955,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_reviews_multi",
            "name": "MTEB AmazonReviewsClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "1399c76144fd37290681b995c656ef9b2e06e26d"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 51.016000000000005,
              "verified": false
            },
            {
              "type": "f1",
              "value": 48.9266470039522,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "arguana",
            "name": "MTEB ArguAna",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 50.806,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/arxiv-clustering-p2p",
            "name": "MTEB ArxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "a122ad7f3f0291bf49cc6f4d32aa80929df69d5d"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 46.19304218375896,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/arxiv-clustering-s2s",
            "name": "MTEB ArxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "f910caf1a6075f7329cdf8c1a6135696f37dbd53"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 37.57785041962193,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/askubuntudupquestions-reranking",
            "name": "MTEB AskUbuntuDupQuestions",
            "config": "default",
            "split": "test",
            "revision": "2000358ca161889fa9c082cb41daa8dcfb161a54"
          },
          "metrics": [
            {
              "type": "map",
              "value": 60.11396377106911,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 72.9068284746955,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/biosses-sts",
            "name": "MTEB BIOSSES",
            "config": "default",
            "split": "test",
            "revision": "d3fb88f8f02e40887cd149695127462bbcf29b4a"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 82.59354737468067,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 81.71933190993215,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 81.39212345994983,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 81.71933190993215,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 81.29257414603093,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 81.80246633432691,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/banking77",
            "name": "MTEB Banking77Classification",
            "config": "default",
            "split": "test",
            "revision": "0fd18e25b25c072e09e0d92ab615fda904d66300"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 79.69805194805193,
              "verified": false
            },
            {
              "type": "f1",
              "value": 79.07431143559548,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/biorxiv-clustering-p2p",
            "name": "MTEB BiorxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "65b79d1d13f80053f67aca9498d9402c2d9f1f40"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 38.973417975095934,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/biorxiv-clustering-s2s",
            "name": "MTEB BiorxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "258694dd0231531bc1fd9de6ceb52a0853c6d908"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 34.51608057107556,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackAndroidRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 46.615,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackEnglishRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 45.383,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackGamingRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 57.062999999999995,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackGisRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 37.201,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackMathematicaRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 27.473,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackPhysicsRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 41.868,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackProgrammersRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 42.059000000000005,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 38.885416666666664,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackStatsRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 32.134,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackTexRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 28.052,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackUnixRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 38.237,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackWebmastersRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 37.875,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "BeIR/cqadupstack",
            "name": "MTEB CQADupstackWordpressRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 32.665,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "climate-fever",
            "name": "MTEB ClimateFEVER",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 28.901,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "dbpedia-entity",
            "name": "MTEB DBPedia",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 41.028,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/emotion",
            "name": "MTEB EmotionClassification",
            "config": "default",
            "split": "test",
            "revision": "4f58c6b202a23cf9a4da393831edf4f9183cad37"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 52.745,
              "verified": false
            },
            {
              "type": "f1",
              "value": 46.432564522368054,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "fever",
            "name": "MTEB FEVER",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 87.64,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "fiqa",
            "name": "MTEB FiQA2018",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 38.834999999999994,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "hotpotqa",
            "name": "MTEB HotpotQA",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 66.793,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/imdb",
            "name": "MTEB ImdbClassification",
            "config": "default",
            "split": "test",
            "revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 92.16680000000001,
              "verified": false
            },
            {
              "type": "ap",
              "value": 88.9326260956379,
              "verified": false
            },
            {
              "type": "f1",
              "value": 92.16197209455585,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "msmarco",
            "name": "MTEB MSMARCO",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 41.325,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/mtop_domain",
            "name": "MTEB MTOPDomainClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 93.62517099863202,
              "verified": false
            },
            {
              "type": "f1",
              "value": 93.3852826127328,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/mtop_intent",
            "name": "MTEB MTOPIntentClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 64.93388052895577,
              "verified": false
            },
            {
              "type": "f1",
              "value": 48.035548201830366,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_massive_intent",
            "name": "MTEB MassiveIntentClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 70.01344989912577,
              "verified": false
            },
            {
              "type": "f1",
              "value": 68.01236893966525,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/amazon_massive_scenario",
            "name": "MTEB MassiveScenarioClassification (en)",
            "config": "en",
            "split": "test",
            "revision": "7d571f92784cd94a019292a1f45445077d0ef634"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 76.34498991257564,
              "verified": false
            },
            {
              "type": "f1",
              "value": 75.72876911765213,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/medrxiv-clustering-p2p",
            "name": "MTEB MedrxivClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "e7a26af6f3ae46b30dde8737f02c07b1505bcc73"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 37.66326759167091,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/medrxiv-clustering-s2s",
            "name": "MTEB MedrxivClusteringS2S",
            "config": "default",
            "split": "test",
            "revision": "35191c8c0dca72d8ff3efcd72aa802307d469663"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 33.53562430544494,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/mind_small",
            "name": "MTEB MindSmallReranking",
            "config": "default",
            "split": "test",
            "revision": "3bdac13927fdc888b903db93b2ffdbd90b295a69"
          },
          "metrics": [
            {
              "type": "map",
              "value": 31.86814320224619,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 33.02567757581291,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "nfcorpus",
            "name": "MTEB NFCorpus",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 33.649,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "nq",
            "name": "MTEB NQ",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 57.994,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "quora",
            "name": "MTEB QuoraRetrieval",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 88.115,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/reddit-clustering",
            "name": "MTEB RedditClustering",
            "config": "default",
            "split": "test",
            "revision": "24640382cdbf8abc73003fb0fa6d111a705499eb"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 53.4970929237201,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/reddit-clustering-p2p",
            "name": "MTEB RedditClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "282350215ef01743dc01b456c7f5241fa8937f16"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 63.59086757472922,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "scidocs",
            "name": "MTEB SCIDOCS",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 18.098,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sickr-sts",
            "name": "MTEB SICK-R",
            "config": "default",
            "split": "test",
            "revision": "a6ea5a8cab320b040a23452cc28066d9beae2cee"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.05019841005287,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 79.65240734965128,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 82.33894047327843,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 79.65240666088022,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 82.33098051639543,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 79.5592521956291,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts12-sts",
            "name": "MTEB STS12",
            "config": "default",
            "split": "test",
            "revision": "a0d554a64d88156834ff5ae9920b964011b16384"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 81.28561469269728,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 72.6022866501722,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 77.89616448619745,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 72.6022866429173,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 77.9073648819866,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 72.6928162672852,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts13-sts",
            "name": "MTEB STS13",
            "config": "default",
            "split": "test",
            "revision": "7e90230a92c190f1bf69ae9002b8cea547a64cca"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 82.48271297318195,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 82.87639489647019,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 82.24654676315204,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 82.87642765399856,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 82.19673632886851,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 82.822727205448,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts14-sts",
            "name": "MTEB STS14",
            "config": "default",
            "split": "test",
            "revision": "6031580fec1f6af667f0bd2da0a551cf4f0b2375"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 83.74140104895864,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 79.74024708732993,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 82.50081856448949,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 79.74024708732993,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 82.36588991657912,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 79.59022658604357,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts15-sts",
            "name": "MTEB STS15",
            "config": "default",
            "split": "test",
            "revision": "ae752c7c21bf194d8b67fd573edf7ae58183cbe3"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 86.30124436614311,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 86.97688974734349,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 86.36868875097032,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 86.97688974734349,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 86.37787059133234,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 86.96666693570158,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts16-sts",
            "name": "MTEB STS16",
            "config": "default",
            "split": "test",
            "revision": "4d8694f8f0e0100860b497b999b3dbed754a0513"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 83.27590066451398,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 84.40811627278994,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 83.77341566536141,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 84.40811627278994,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 83.72567664904311,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 84.42172336387632,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts17-crosslingual-sts",
            "name": "MTEB STS17 (en-en)",
            "config": "en-en",
            "split": "test",
            "revision": "af5e6fb845001ecf41f4c1e033ce921939a2a68d"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 89.13791942173916,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 89.22016928873572,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 89.43583792557924,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 89.22016928873572,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 89.47307915863284,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 89.20752264220539,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/sts22-crosslingual-sts",
            "name": "MTEB STS22 (en)",
            "config": "en",
            "split": "test",
            "revision": "6d1ba47164174a496b7fa5d3569dae26a6813b80"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 64.92003328655028,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 65.42027229611072,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 66.68765284942059,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 65.42027229611072,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 66.85383496796447,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 65.53490117706689,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "STS"
          },
          "dataset": {
            "type": "mteb/stsbenchmark-sts",
            "name": "MTEB STSBenchmark",
            "config": "default",
            "split": "test",
            "revision": "b0fddb56ed78048fa8b90373c8a3cfc37b684831"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 85.97445894753297,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 86.57651994952795,
              "verified": false
            },
            {
              "type": "euclidean_pearson",
              "value": 86.7061296897819,
              "verified": false
            },
            {
              "type": "euclidean_spearman",
              "value": 86.57651994952795,
              "verified": false
            },
            {
              "type": "manhattan_pearson",
              "value": 86.66411668551642,
              "verified": false
            },
            {
              "type": "manhattan_spearman",
              "value": 86.53200653755397,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/scidocs-reranking",
            "name": "MTEB SciDocsRR",
            "config": "default",
            "split": "test",
            "revision": "d3c5e1fc0b855ab6097bf1cda04dd73947d7caab"
          },
          "metrics": [
            {
              "type": "map",
              "value": 81.62235389081138,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 94.65811965811966,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "scifact",
            "name": "MTEB SciFact",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 66.687,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/sprintduplicatequestions-pairclassification",
            "name": "MTEB SprintDuplicateQuestions",
            "config": "default",
            "split": "test",
            "revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 99.86435643564356,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 96.59150882873165,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 93.07030854830552,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 94.16581371545547,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 92,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 99.86435643564356,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 96.59150882873165,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 93.07030854830552,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 94.16581371545547,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 92,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 99.86435643564356,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 96.59150882873162,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 93.07030854830552,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 94.16581371545547,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 92,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 99.86336633663366,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 96.58123246795022,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 92.9591836734694,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 94.89583333333333,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 91.10000000000001,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 99.86435643564356,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 96.59150882873165,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 93.07030854830552,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/stackexchange-clustering",
            "name": "MTEB StackExchangeClustering",
            "config": "default",
            "split": "test",
            "revision": "6cbc1f7b2bc0622f2e39d2c77fa502909748c259"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 62.938055854344455,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/stackexchange-clustering-p2p",
            "name": "MTEB StackExchangeClusteringP2P",
            "config": "default",
            "split": "test",
            "revision": "815ca46b2622cec33ccafc3735d572c266efdb44"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 36.479716154538224,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Reranking"
          },
          "dataset": {
            "type": "mteb/stackoverflowdupquestions-reranking",
            "name": "MTEB StackOverflowDupQuestions",
            "config": "default",
            "split": "test",
            "revision": "e185fbe320c72810689fc5848eb6114e1ef5ec69"
          },
          "metrics": [
            {
              "type": "map",
              "value": 50.75827388766867,
              "verified": false
            },
            {
              "type": "mrr",
              "value": 51.65291305916306,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Summarization"
          },
          "dataset": {
            "type": "mteb/summeval",
            "name": "MTEB SummEval",
            "config": "default",
            "split": "test",
            "revision": "cda12ad7615edc362dbf25a00fdd61d3b1eaf93c"
          },
          "metrics": [
            {
              "type": "cos_sim_pearson",
              "value": 31.81419421090782,
              "verified": false
            },
            {
              "type": "cos_sim_spearman",
              "value": 31.287464634068492,
              "verified": false
            },
            {
              "type": "dot_pearson",
              "value": 31.814195589790177,
              "verified": false
            },
            {
              "type": "dot_spearman",
              "value": 31.287464634068492,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "trec-covid",
            "name": "MTEB TRECCOVID",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 79.364,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Retrieval"
          },
          "dataset": {
            "type": "webis-touche2020",
            "name": "MTEB Touche2020",
            "config": "default",
            "split": "test",
            "revision": "None"
          },
          "metrics": [
            {
              "type": "ndcg_at_10",
              "value": 31.927,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/toxic_conversations_50k",
            "name": "MTEB ToxicConversationsClassification",
            "config": "default",
            "split": "test",
            "revision": "d7c0de2777da35d6aae2200a62c6e0e5af397c4c"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 73.0414,
              "verified": false
            },
            {
              "type": "ap",
              "value": 16.06723077348852,
              "verified": false
            },
            {
              "type": "f1",
              "value": 56.73470421774399,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Classification"
          },
          "dataset": {
            "type": "mteb/tweet_sentiment_extraction",
            "name": "MTEB TweetSentimentExtractionClassification",
            "config": "default",
            "split": "test",
            "revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a"
          },
          "metrics": [
            {
              "type": "accuracy",
              "value": 64.72269383135257,
              "verified": false
            },
            {
              "type": "f1",
              "value": 64.70143593421479,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "Clustering"
          },
          "dataset": {
            "type": "mteb/twentynewsgroups-clustering",
            "name": "MTEB TwentyNewsgroupsClustering",
            "config": "default",
            "split": "test",
            "revision": "6125ec4e24fa026cec8a478383ee943acfbd5449"
          },
          "metrics": [
            {
              "type": "v_measure",
              "value": 46.06343037695152,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/twittersemeval2015-pairclassification",
            "name": "MTEB TwitterSemEval2015",
            "config": "default",
            "split": "test",
            "revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 85.59337187816654,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 72.23331527941706,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 67.22915138175593,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 62.64813126709207,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 72.53298153034301,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 85.59337187816654,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 72.23332517262921,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 67.22915138175593,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 62.64813126709207,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 72.53298153034301,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 85.59337187816654,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 72.23331029091486,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 67.22915138175593,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 62.64813126709207,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 72.53298153034301,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 85.4622399713894,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 72.05180729774357,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 67.12683347713546,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 62.98866527874162,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 71.84696569920844,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 85.59337187816654,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 72.23332517262921,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 67.22915138175593,
              "verified": false
            }
          ]
        },
        {
          "task": {
            "type": "PairClassification"
          },
          "dataset": {
            "type": "mteb/twitterurlcorpus-pairclassification",
            "name": "MTEB TwitterURLCorpus",
            "config": "default",
            "split": "test",
            "revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf"
          },
          "metrics": [
            {
              "type": "cos_sim_accuracy",
              "value": 89.08681647067955,
              "verified": false
            },
            {
              "type": "cos_sim_ap",
              "value": 86.31913876322757,
              "verified": false
            },
            {
              "type": "cos_sim_f1",
              "value": 78.678007640741,
              "verified": false
            },
            {
              "type": "cos_sim_precision",
              "value": 73.95988616343678,
              "verified": false
            },
            {
              "type": "cos_sim_recall",
              "value": 84.03911302740991,
              "verified": false
            },
            {
              "type": "dot_accuracy",
              "value": 89.08681647067955,
              "verified": false
            },
            {
              "type": "dot_ap",
              "value": 86.31913976395484,
              "verified": false
            },
            {
              "type": "dot_f1",
              "value": 78.678007640741,
              "verified": false
            },
            {
              "type": "dot_precision",
              "value": 73.95988616343678,
              "verified": false
            },
            {
              "type": "dot_recall",
              "value": 84.03911302740991,
              "verified": false
            },
            {
              "type": "euclidean_accuracy",
              "value": 89.08681647067955,
              "verified": false
            },
            {
              "type": "euclidean_ap",
              "value": 86.31913869004254,
              "verified": false
            },
            {
              "type": "euclidean_f1",
              "value": 78.678007640741,
              "verified": false
            },
            {
              "type": "euclidean_precision",
              "value": 73.95988616343678,
              "verified": false
            },
            {
              "type": "euclidean_recall",
              "value": 84.03911302740991,
              "verified": false
            },
            {
              "type": "manhattan_accuracy",
              "value": 89.06547133930997,
              "verified": false
            },
            {
              "type": "manhattan_ap",
              "value": 86.24122868846949,
              "verified": false
            },
            {
              "type": "manhattan_f1",
              "value": 78.74963094183643,
              "verified": false
            },
            {
              "type": "manhattan_precision",
              "value": 75.62375956903884,
              "verified": false
            },
            {
              "type": "manhattan_recall",
              "value": 82.14505697566985,
              "verified": false
            },
            {
              "type": "max_accuracy",
              "value": 89.08681647067955,
              "verified": false
            },
            {
              "type": "max_ap",
              "value": 86.31913976395484,
              "verified": false
            },
            {
              "type": "max_f1",
              "value": 78.74963094183643,
              "verified": false
            }
          ]
        }
      ]
    }
  ],
  "config": {
    "tokenizer_config": {
      "cls_token": "[CLS]",
      "mask_token": "[MASK]",
      "pad_token": "[PAD]",
      "sep_token": "[SEP]",
      "unk_token": "[UNK]"
    }
  },
  "cardData": {
    "tags": [
      "mteb"
    ],
    "model-index": [
      {
        "name": "embed-english-light-v3.0",
        "results": [
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_counterfactual",
              "name": "MTEB AmazonCounterfactualClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "e8379541af4e31359cca9fbcf4b00f2671dba205"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 78.62686567164178,
                "verified": false
              },
              {
                "type": "ap",
                "value": 43.50072127690769,
                "verified": false
              },
              {
                "type": "f1",
                "value": 73.12414870629323,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_polarity",
              "name": "MTEB AmazonPolarityClassification",
              "config": "default",
              "split": "test",
              "revision": "e2d317d38cd51312af73b3d32a06d1a08b442046"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 94.795,
                "verified": false
              },
              {
                "type": "ap",
                "value": 92.14178233328848,
                "verified": false
              },
              {
                "type": "f1",
                "value": 94.79269356571955,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_reviews_multi",
              "name": "MTEB AmazonReviewsClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "1399c76144fd37290681b995c656ef9b2e06e26d"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 51.016000000000005,
                "verified": false
              },
              {
                "type": "f1",
                "value": 48.9266470039522,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "arguana",
              "name": "MTEB ArguAna",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 50.806,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/arxiv-clustering-p2p",
              "name": "MTEB ArxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "a122ad7f3f0291bf49cc6f4d32aa80929df69d5d"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 46.19304218375896,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/arxiv-clustering-s2s",
              "name": "MTEB ArxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "f910caf1a6075f7329cdf8c1a6135696f37dbd53"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 37.57785041962193,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/askubuntudupquestions-reranking",
              "name": "MTEB AskUbuntuDupQuestions",
              "config": "default",
              "split": "test",
              "revision": "2000358ca161889fa9c082cb41daa8dcfb161a54"
            },
            "metrics": [
              {
                "type": "map",
                "value": 60.11396377106911,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 72.9068284746955,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/biosses-sts",
              "name": "MTEB BIOSSES",
              "config": "default",
              "split": "test",
              "revision": "d3fb88f8f02e40887cd149695127462bbcf29b4a"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 82.59354737468067,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 81.71933190993215,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 81.39212345994983,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 81.71933190993215,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 81.29257414603093,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 81.80246633432691,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/banking77",
              "name": "MTEB Banking77Classification",
              "config": "default",
              "split": "test",
              "revision": "0fd18e25b25c072e09e0d92ab615fda904d66300"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 79.69805194805193,
                "verified": false
              },
              {
                "type": "f1",
                "value": 79.07431143559548,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/biorxiv-clustering-p2p",
              "name": "MTEB BiorxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "65b79d1d13f80053f67aca9498d9402c2d9f1f40"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 38.973417975095934,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/biorxiv-clustering-s2s",
              "name": "MTEB BiorxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "258694dd0231531bc1fd9de6ceb52a0853c6d908"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 34.51608057107556,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackAndroidRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 46.615,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackEnglishRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 45.383,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackGamingRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 57.062999999999995,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackGisRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 37.201,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackMathematicaRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 27.473,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackPhysicsRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 41.868,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackProgrammersRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 42.059000000000005,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 38.885416666666664,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackStatsRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 32.134,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackTexRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 28.052,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackUnixRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 38.237,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackWebmastersRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 37.875,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "BeIR/cqadupstack",
              "name": "MTEB CQADupstackWordpressRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 32.665,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "climate-fever",
              "name": "MTEB ClimateFEVER",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 28.901,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "dbpedia-entity",
              "name": "MTEB DBPedia",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 41.028,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/emotion",
              "name": "MTEB EmotionClassification",
              "config": "default",
              "split": "test",
              "revision": "4f58c6b202a23cf9a4da393831edf4f9183cad37"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 52.745,
                "verified": false
              },
              {
                "type": "f1",
                "value": 46.432564522368054,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "fever",
              "name": "MTEB FEVER",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 87.64,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "fiqa",
              "name": "MTEB FiQA2018",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 38.834999999999994,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "hotpotqa",
              "name": "MTEB HotpotQA",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 66.793,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/imdb",
              "name": "MTEB ImdbClassification",
              "config": "default",
              "split": "test",
              "revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 92.16680000000001,
                "verified": false
              },
              {
                "type": "ap",
                "value": 88.9326260956379,
                "verified": false
              },
              {
                "type": "f1",
                "value": 92.16197209455585,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "msmarco",
              "name": "MTEB MSMARCO",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 41.325,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/mtop_domain",
              "name": "MTEB MTOPDomainClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 93.62517099863202,
                "verified": false
              },
              {
                "type": "f1",
                "value": 93.3852826127328,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/mtop_intent",
              "name": "MTEB MTOPIntentClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 64.93388052895577,
                "verified": false
              },
              {
                "type": "f1",
                "value": 48.035548201830366,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_massive_intent",
              "name": "MTEB MassiveIntentClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 70.01344989912577,
                "verified": false
              },
              {
                "type": "f1",
                "value": 68.01236893966525,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/amazon_massive_scenario",
              "name": "MTEB MassiveScenarioClassification (en)",
              "config": "en",
              "split": "test",
              "revision": "7d571f92784cd94a019292a1f45445077d0ef634"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 76.34498991257564,
                "verified": false
              },
              {
                "type": "f1",
                "value": 75.72876911765213,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/medrxiv-clustering-p2p",
              "name": "MTEB MedrxivClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "e7a26af6f3ae46b30dde8737f02c07b1505bcc73"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 37.66326759167091,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/medrxiv-clustering-s2s",
              "name": "MTEB MedrxivClusteringS2S",
              "config": "default",
              "split": "test",
              "revision": "35191c8c0dca72d8ff3efcd72aa802307d469663"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 33.53562430544494,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/mind_small",
              "name": "MTEB MindSmallReranking",
              "config": "default",
              "split": "test",
              "revision": "3bdac13927fdc888b903db93b2ffdbd90b295a69"
            },
            "metrics": [
              {
                "type": "map",
                "value": 31.86814320224619,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 33.02567757581291,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "nfcorpus",
              "name": "MTEB NFCorpus",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 33.649,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "nq",
              "name": "MTEB NQ",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 57.994,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "quora",
              "name": "MTEB QuoraRetrieval",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 88.115,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/reddit-clustering",
              "name": "MTEB RedditClustering",
              "config": "default",
              "split": "test",
              "revision": "24640382cdbf8abc73003fb0fa6d111a705499eb"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 53.4970929237201,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/reddit-clustering-p2p",
              "name": "MTEB RedditClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "282350215ef01743dc01b456c7f5241fa8937f16"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 63.59086757472922,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "scidocs",
              "name": "MTEB SCIDOCS",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 18.098,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sickr-sts",
              "name": "MTEB SICK-R",
              "config": "default",
              "split": "test",
              "revision": "a6ea5a8cab320b040a23452cc28066d9beae2cee"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.05019841005287,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 79.65240734965128,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 82.33894047327843,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 79.65240666088022,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 82.33098051639543,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 79.5592521956291,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts12-sts",
              "name": "MTEB STS12",
              "config": "default",
              "split": "test",
              "revision": "a0d554a64d88156834ff5ae9920b964011b16384"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 81.28561469269728,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 72.6022866501722,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 77.89616448619745,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 72.6022866429173,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 77.9073648819866,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 72.6928162672852,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts13-sts",
              "name": "MTEB STS13",
              "config": "default",
              "split": "test",
              "revision": "7e90230a92c190f1bf69ae9002b8cea547a64cca"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 82.48271297318195,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 82.87639489647019,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 82.24654676315204,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 82.87642765399856,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 82.19673632886851,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 82.822727205448,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts14-sts",
              "name": "MTEB STS14",
              "config": "default",
              "split": "test",
              "revision": "6031580fec1f6af667f0bd2da0a551cf4f0b2375"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 83.74140104895864,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 79.74024708732993,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 82.50081856448949,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 79.74024708732993,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 82.36588991657912,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 79.59022658604357,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts15-sts",
              "name": "MTEB STS15",
              "config": "default",
              "split": "test",
              "revision": "ae752c7c21bf194d8b67fd573edf7ae58183cbe3"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 86.30124436614311,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 86.97688974734349,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 86.36868875097032,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 86.97688974734349,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 86.37787059133234,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 86.96666693570158,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts16-sts",
              "name": "MTEB STS16",
              "config": "default",
              "split": "test",
              "revision": "4d8694f8f0e0100860b497b999b3dbed754a0513"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 83.27590066451398,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 84.40811627278994,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 83.77341566536141,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 84.40811627278994,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 83.72567664904311,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 84.42172336387632,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts17-crosslingual-sts",
              "name": "MTEB STS17 (en-en)",
              "config": "en-en",
              "split": "test",
              "revision": "af5e6fb845001ecf41f4c1e033ce921939a2a68d"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 89.13791942173916,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 89.22016928873572,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 89.43583792557924,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 89.22016928873572,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 89.47307915863284,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 89.20752264220539,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/sts22-crosslingual-sts",
              "name": "MTEB STS22 (en)",
              "config": "en",
              "split": "test",
              "revision": "6d1ba47164174a496b7fa5d3569dae26a6813b80"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 64.92003328655028,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 65.42027229611072,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 66.68765284942059,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 65.42027229611072,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 66.85383496796447,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 65.53490117706689,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "STS"
            },
            "dataset": {
              "type": "mteb/stsbenchmark-sts",
              "name": "MTEB STSBenchmark",
              "config": "default",
              "split": "test",
              "revision": "b0fddb56ed78048fa8b90373c8a3cfc37b684831"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 85.97445894753297,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 86.57651994952795,
                "verified": false
              },
              {
                "type": "euclidean_pearson",
                "value": 86.7061296897819,
                "verified": false
              },
              {
                "type": "euclidean_spearman",
                "value": 86.57651994952795,
                "verified": false
              },
              {
                "type": "manhattan_pearson",
                "value": 86.66411668551642,
                "verified": false
              },
              {
                "type": "manhattan_spearman",
                "value": 86.53200653755397,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/scidocs-reranking",
              "name": "MTEB SciDocsRR",
              "config": "default",
              "split": "test",
              "revision": "d3c5e1fc0b855ab6097bf1cda04dd73947d7caab"
            },
            "metrics": [
              {
                "type": "map",
                "value": 81.62235389081138,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 94.65811965811966,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "scifact",
              "name": "MTEB SciFact",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 66.687,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/sprintduplicatequestions-pairclassification",
              "name": "MTEB SprintDuplicateQuestions",
              "config": "default",
              "split": "test",
              "revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 99.86435643564356,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 96.59150882873165,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 93.07030854830552,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 94.16581371545547,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 92,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 99.86435643564356,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 96.59150882873165,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 93.07030854830552,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 94.16581371545547,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 92,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 99.86435643564356,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 96.59150882873162,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 93.07030854830552,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 94.16581371545547,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 92,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 99.86336633663366,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 96.58123246795022,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 92.9591836734694,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 94.89583333333333,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 91.10000000000001,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 99.86435643564356,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 96.59150882873165,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 93.07030854830552,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/stackexchange-clustering",
              "name": "MTEB StackExchangeClustering",
              "config": "default",
              "split": "test",
              "revision": "6cbc1f7b2bc0622f2e39d2c77fa502909748c259"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 62.938055854344455,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/stackexchange-clustering-p2p",
              "name": "MTEB StackExchangeClusteringP2P",
              "config": "default",
              "split": "test",
              "revision": "815ca46b2622cec33ccafc3735d572c266efdb44"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 36.479716154538224,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Reranking"
            },
            "dataset": {
              "type": "mteb/stackoverflowdupquestions-reranking",
              "name": "MTEB StackOverflowDupQuestions",
              "config": "default",
              "split": "test",
              "revision": "e185fbe320c72810689fc5848eb6114e1ef5ec69"
            },
            "metrics": [
              {
                "type": "map",
                "value": 50.75827388766867,
                "verified": false
              },
              {
                "type": "mrr",
                "value": 51.65291305916306,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Summarization"
            },
            "dataset": {
              "type": "mteb/summeval",
              "name": "MTEB SummEval",
              "config": "default",
              "split": "test",
              "revision": "cda12ad7615edc362dbf25a00fdd61d3b1eaf93c"
            },
            "metrics": [
              {
                "type": "cos_sim_pearson",
                "value": 31.81419421090782,
                "verified": false
              },
              {
                "type": "cos_sim_spearman",
                "value": 31.287464634068492,
                "verified": false
              },
              {
                "type": "dot_pearson",
                "value": 31.814195589790177,
                "verified": false
              },
              {
                "type": "dot_spearman",
                "value": 31.287464634068492,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "trec-covid",
              "name": "MTEB TRECCOVID",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 79.364,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Retrieval"
            },
            "dataset": {
              "type": "webis-touche2020",
              "name": "MTEB Touche2020",
              "config": "default",
              "split": "test",
              "revision": "None"
            },
            "metrics": [
              {
                "type": "ndcg_at_10",
                "value": 31.927,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/toxic_conversations_50k",
              "name": "MTEB ToxicConversationsClassification",
              "config": "default",
              "split": "test",
              "revision": "d7c0de2777da35d6aae2200a62c6e0e5af397c4c"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 73.0414,
                "verified": false
              },
              {
                "type": "ap",
                "value": 16.06723077348852,
                "verified": false
              },
              {
                "type": "f1",
                "value": 56.73470421774399,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Classification"
            },
            "dataset": {
              "type": "mteb/tweet_sentiment_extraction",
              "name": "MTEB TweetSentimentExtractionClassification",
              "config": "default",
              "split": "test",
              "revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 64.72269383135257,
                "verified": false
              },
              {
                "type": "f1",
                "value": 64.70143593421479,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "Clustering"
            },
            "dataset": {
              "type": "mteb/twentynewsgroups-clustering",
              "name": "MTEB TwentyNewsgroupsClustering",
              "config": "default",
              "split": "test",
              "revision": "6125ec4e24fa026cec8a478383ee943acfbd5449"
            },
            "metrics": [
              {
                "type": "v_measure",
                "value": 46.06343037695152,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/twittersemeval2015-pairclassification",
              "name": "MTEB TwitterSemEval2015",
              "config": "default",
              "split": "test",
              "revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 85.59337187816654,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 72.23331527941706,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 67.22915138175593,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 62.64813126709207,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 72.53298153034301,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 85.59337187816654,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 72.23332517262921,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 67.22915138175593,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 62.64813126709207,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 72.53298153034301,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 85.59337187816654,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 72.23331029091486,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 67.22915138175593,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 62.64813126709207,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 72.53298153034301,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 85.4622399713894,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 72.05180729774357,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 67.12683347713546,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 62.98866527874162,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 71.84696569920844,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 85.59337187816654,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 72.23332517262921,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 67.22915138175593,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "PairClassification"
            },
            "dataset": {
              "type": "mteb/twitterurlcorpus-pairclassification",
              "name": "MTEB TwitterURLCorpus",
              "config": "default",
              "split": "test",
              "revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf"
            },
            "metrics": [
              {
                "type": "cos_sim_accuracy",
                "value": 89.08681647067955,
                "verified": false
              },
              {
                "type": "cos_sim_ap",
                "value": 86.31913876322757,
                "verified": false
              },
              {
                "type": "cos_sim_f1",
                "value": 78.678007640741,
                "verified": false
              },
              {
                "type": "cos_sim_precision",
                "value": 73.95988616343678,
                "verified": false
              },
              {
                "type": "cos_sim_recall",
                "value": 84.03911302740991,
                "verified": false
              },
              {
                "type": "dot_accuracy",
                "value": 89.08681647067955,
                "verified": false
              },
              {
                "type": "dot_ap",
                "value": 86.31913976395484,
                "verified": false
              },
              {
                "type": "dot_f1",
                "value": 78.678007640741,
                "verified": false
              },
              {
                "type": "dot_precision",
                "value": 73.95988616343678,
                "verified": false
              },
              {
                "type": "dot_recall",
                "value": 84.03911302740991,
                "verified": false
              },
              {
                "type": "euclidean_accuracy",
                "value": 89.08681647067955,
                "verified": false
              },
              {
                "type": "euclidean_ap",
                "value": 86.31913869004254,
                "verified": false
              },
              {
                "type": "euclidean_f1",
                "value": 78.678007640741,
                "verified": false
              },
              {
                "type": "euclidean_precision",
                "value": 73.95988616343678,
                "verified": false
              },
              {
                "type": "euclidean_recall",
                "value": 84.03911302740991,
                "verified": false
              },
              {
                "type": "manhattan_accuracy",
                "value": 89.06547133930997,
                "verified": false
              },
              {
                "type": "manhattan_ap",
                "value": 86.24122868846949,
                "verified": false
              },
              {
                "type": "manhattan_f1",
                "value": 78.74963094183643,
                "verified": false
              },
              {
                "type": "manhattan_precision",
                "value": 75.62375956903884,
                "verified": false
              },
              {
                "type": "manhattan_recall",
                "value": 82.14505697566985,
                "verified": false
              },
              {
                "type": "max_accuracy",
                "value": 89.08681647067955,
                "verified": false
              },
              {
                "type": "max_ap",
                "value": 86.31913976395484,
                "verified": false
              },
              {
                "type": "max_f1",
                "value": 78.74963094183643,
                "verified": false
              }
            ]
          }
        ]
      }
    ]
  },
  "transformersInfo": {
    "auto_model": "AutoModel"
  },
  "siblings": [
    {
      "rfilename": ".gitattributes"
    },
    {
      "rfilename": "README.md"
    },
    {
      "rfilename": "added_tokens.json"
    },
    {
      "rfilename": "config.json"
    },
    {
      "rfilename": "special_tokens_map.json"
    },
    {
      "rfilename": "tokenizer.json"
    },
    {
      "rfilename": "tokenizer_config.json"
    },
    {
      "rfilename": "vocab.txt"
    }
  ],
  "spaces": [
    "mteb/leaderboard",
    "mteb/leaderboard_legacy",
    "m-ric/Quotes",
    "SmileXing/leaderboard",
    "sq66/leaderboard_legacy",
    "q275343119/leaderboard",
    "AyushM6/leaderboard",
    "shiwan7788/leaderboard",
    "Chengyanci/11",
    "yanciyuyu/1",
    "n8n-1/8",
    "reader-1/1"
  ],
  "createdAt": "2023-11-02T10:05:45.000Z",
  "usedStorage": 0
}