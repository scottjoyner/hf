
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generate SQL Server schema + bulkload scripts from a SQLite database.
Outputs go to ../generated by default.

Usage:
  python generate_mssql_from_sqlite.py --sqlite /path/to/app.db --out ../generated --db appdb
"""
import argparse, os, sqlite3, re, json
from pathlib import Path
from collections import defaultdict, deque

def type_affinity(t):
  if t is None: return "BLOB"
  T = t.upper()
  if "INT" in T:
    return "INTEGER"
  if any(x in T for x in ["CHAR", "CLOB", "TEXT"]):
    return "TEXT"
  if "BLOB" in T or T.strip() == "":
    return "BLOB"
  if any(x in T for x in ["REAL", "FLOA", "DOUB"]):
    return "REAL"
  return "NUMERIC"

def parse_varchar_len(t):
  if not t: return None
  m = re.search(r"(?:VAR)?CHAR\s*\((\d+)\)", t, re.IGNORECASE)
  if m:
    try: return int(m.group(1))
    except: return None
  return None

def map_sqlite_type_to_mssql(colname, decltype, is_pk):
  T = (decltype or "").upper()
  aff = type_affinity(T)
  if any(x in T for x in ["DATE", "TIME"]):
    return "DATETIME2(3)"
  m = re.search(r"DECIMAL\s*\((\d+)\s*,\s*(\d+)\)", T)
  if m:
    p,s = int(m.group(1)), int(m.group(2))
    p = max(1, min(p,38)); s = max(0, min(s, p-1))
    return f"DECIMAL({p},{s})"
  if aff == "NUMERIC":
    return "DECIMAL(18,6)"
  if aff == "REAL":
    return "FLOAT"
  if aff == "BLOB":
    return "VARBINARY(MAX)"
  if aff == "TEXT":
    n = parse_varchar_len(T)
    if n and n <= 4000:
      return f"NVARCHAR({n})"
    return "NVARCHAR(MAX)"
  if aff == "INTEGER":
    if is_pk:
      return "BIGINT"
    if re.match(r"^(is_|has_|flag_)", colname, re.IGNORECASE):
      return "BIT"
    return "INT"
  return "NVARCHAR(MAX)"

def get_tables(conn):
  cur = conn.execute("SELECT name, sql FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
  return [{"name": r[0], "sql": r[1]} for r in cur.fetchall()]

def get_columns(conn, table):
  cur = conn.execute(f"PRAGMA table_info('{table}')")
  cols = []
  for cid, name, ctype, notnull, dflt, pk in cur.fetchall():
    cols.append({"cid": cid, "name": name, "type": ctype, "notnull": bool(notnull), "default": dflt, "pk": int(pk)})
  return cols

def get_indexes(conn, table):
  idx = []
  for (name, unique, origin, partial) in conn.execute(f"PRAGMA index_list('{table}')"):
    info = {"name": name, "unique": bool(unique), "cols": []}
    for seqno, cid, colname in conn.execute(f"PRAGMA index_info('{name}')"):
      info["cols"].append(colname)
    if name.startswith("sqlite_autoindex"):
      continue
    idx.append(info)
  return idx

def get_fks(conn, table):
  fks = []
  for row in conn.execute(f"PRAGMA foreign_key_list('{table}')"):
    d = {"table": row[2], "from": row[3], "to": row[4], "on_update": row[5], "on_delete": row[6], "match": row[7]}
    fks.append(d)
  return fks

def topo_sort(tables, fk_map):
  indeg = {t:0 for t in tables}
  adj = defaultdict(list)
  for child, fks in fk_map.items():
    for fk in fks:
      parent = fk["table"]
      if parent in indeg and child in indeg:
        adj[parent].append(child)
        indeg[child]+=1
  from collections import deque
  q = deque([t for t,d in indeg.items() if d==0])
  order = []
  while q:
    u = q.popleft()
    order.append(u)
    for v in adj[u]:
      indeg[v]-=1
      if indeg[v]==0:
        q.append(v)
  for t,d in indeg.items():
    if t not in order:
      order.append(t)
  return order

def main():
  ap = argparse.ArgumentParser()
  ap.add_argument("--sqlite", required=True)
  ap.add_argument("--out", required=True)
  ap.add_argument("--db", default="appdb")
  args = ap.parse_args()

  Path(args.out).mkdir(parents=True, exist_ok=True)
  conn = sqlite3.connect(args.sqlite)
  conn.row_factory = sqlite3.Row

  tables = [t["name"] for t in get_tables(conn)]
  fk_map = {t: get_fks(conn, t) for t in tables}
  order = topo_sort(tables, fk_map)

  ddl_lines = [f"USE [{args.db}];", "GO", "SET QUOTED_IDENTIFIER ON;", "GO"]
  fk_deferred = []
  idx_lines = []

  for t in order:
    cols = get_columns(conn, t)
    pks = [c for c in cols if c["pk"]>0]
    single_int_pk = len(pks)==1 and type_affinity(pks[0]["type"])=="INTEGER"
    coldefs = []
    for c in cols:
      mssql_type = map_sqlite_type_to_mssql(c["name"], c["type"], is_pk=(c in pks))
      nullness = "NOT NULL" if c["notnull"] or (c in pks) else "NULL"
      default = ""
      if c["default"] is not None:
        dv = str(c["default"]).strip()
        if dv.startswith("(") and dv.endswith(")"):
          dv = dv[1:-1]
        default = f" DEFAULT ({dv})"
      identity = ""
      if single_int_pk and c["name"]==pks[0]["name"]:
        identity = " IDENTITY(1,1)"
      coldefs.append(f"  [{c['name']}] {mssql_type}{identity} {nullness}{default}")
    pk_clause = ""
    if pks:
      pk_cols = ", ".join(f"[{c['name']}]" for c in sorted(pks, key=lambda x:x["pk"]))
      pk_clause = f",
  CONSTRAINT [PK_{t}] PRIMARY KEY CLUSTERED ({pk_cols})"

    ddl_lines.append(f"IF OBJECT_ID('dbo.[{t}]','U') IS NOT NULL DROP TABLE dbo.[{t}];")
    ddl_lines.append("GO")
    ddl_lines.append(f"CREATE TABLE dbo.[{t}] (
" + ",
".join(coldefs) + (pk_clause if pk_clause else "") + "
);")
    ddl_lines.append("GO")

    for idx in get_indexes(conn, t):
      idxname = f"IX_{t}_{'_'.join(idx['cols'])}"
      cols_expr = ", ".join(f"[{c}]" for c in idx["cols"])
      if idx["unique"]:
        idx_lines.append(f"CREATE UNIQUE INDEX [{idxname}] ON dbo.[{t}] ({cols_expr});")
      else:
        idx_lines.append(f"CREATE INDEX [{idxname}] ON dbo.[{t}] ({cols_expr});")

    for fk in fk_map[t]:
      ref_t = fk["table"]
      fkname = f"FK_{t}_{fk['from']}_{ref_t}_{fk['to']}"
      ondel = f" ON DELETE {fk['on_delete'].upper()}" if fk.get("on_delete") and fk["on_delete"]!="NO ACTION" else ""
      onupd = f" ON UPDATE {fk['on_update'].upper()}" if fk.get("on_update") and fk["on_update"]!="NO ACTION" else ""
      fk_deferred.append(f"ALTER TABLE dbo.[{t}] WITH CHECK ADD CONSTRAINT [{fkname}] FOREIGN KEY ([{fk['from']}]) REFERENCES dbo.[{ref_t}]([{fk['to']}]){ondel}{onupd};
ALTER TABLE dbo.[{t}] CHECK CONSTRAINT [{fkname}];")

  with open(Path(args.out)/"01_schema_generated.sql", "w", encoding="utf-8") as f:
    f.write("
".join(ddl_lines))
    if idx_lines:
      f.write("
GO
")
      f.write("
".join(idx_lines))
      f.write("
GO
")

  with open(Path(args.out)/"02_constraints_generated.sql", "w", encoding="utf-8") as f:
    f.write(f"USE [{args.db}];
GO
SET QUOTED_IDENTIFIER ON;
GO
")
    for line in fk_deferred:
      f.write(line + "
GO
")

  bl = [f"USE [{args.db}];", "GO"]
  for t in order:
    bl.append(f"-- Stage for {t}")
    bl.append(f"IF OBJECT_ID('dbo.[{t}]_stage','U') IS NOT NULL DROP TABLE dbo.[{t}]_stage;")
    bl.append(f"SELECT TOP 0 * INTO dbo.[{t}]_stage FROM dbo.[{t}];")
    bl.append("GO")
    bl.append(f"BULK INSERT dbo.[{t}]_stage FROM '/var/opt/mssql/import/{t}.csv' WITH (FIRSTROW=2, FIELDTERMINATOR=',', ROWTERMINATOR='0x0a', TABLOCK, CODEPAGE='65001');")
    bl.append("GO")
    bl.append(f"DECLARE @has_identity bit = CASE WHEN EXISTS (SELECT 1 FROM sys.columns WHERE object_id = OBJECT_ID('dbo.[{t}]') AND is_identity = 1) THEN 1 ELSE 0 END;")
    col_list = ", ".join(f"[{c['name']}]" for c in get_columns(conn, t))
    bl.append(f"IF @has_identity = 1 SET IDENTITY_INSERT dbo.[{t}] ON;")
    bl.append(f"INSERT INTO dbo.[{t}] ({col_list}) SELECT {col_list} FROM dbo.[{t}]_stage;")
    bl.append(f"IF @has_identity = 1 SET IDENTITY_INSERT dbo.[{t}] OFF;")
    bl.append(f"DROP TABLE dbo.[{t}]_stage;")
    bl.append("GO")
  with open(Path(args.out)/"03_bulkload_generated.sql", "w", encoding="utf-8") as f:
    f.write("
".join(bl))

  with open(Path(args.out)/"table_order.json", "w", encoding="utf-8") as f:
    json.dump({"load_order": order}, f, indent=2)

  print(f"âœ” Generated scripts in {args.out}")
if __name__ == "__main__":
  main()
